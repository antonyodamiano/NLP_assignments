{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_assignments/blob/carlo/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GM9DBN-Qz3k"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Fact checking, Neural Languange Inference (**NLI**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO_-4CZeRCO7"
      },
      "source": [
        "# Intro\n",
        "\n",
        "This assignment is centred on a particular and emerging NLP task, formally known as **fact checking** (or fake checking). As AI techniques become more and more powerful, reaching amazing results, such as image and text generation, it is more than ever necessary to build tools able to distinguish what is real from what is fake.\n",
        "\n",
        "Here we focus on a small portion of the whole fact checking problem, which aims to determine whether a given statement (fact) conveys a trustworthy information or not. \n",
        "\n",
        "More precisely, given a set of evidences and a fact to verify, we would like our model to correctly predict whether the fact is true or fake.\n",
        "\n",
        "In particular, we will see:\n",
        "\n",
        "*   Dataset preparation (analysis and pre-processing)\n",
        "*   Problem formulation: multi-input binary classification\n",
        "*   Defining an evaluation method\n",
        "*   Simple sentence embedding\n",
        "*   Neural building blocks\n",
        "*   Neural architecture extension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGDwg78PS_uy"
      },
      "source": [
        "# The FEVER dataset\n",
        "\n",
        "First of all, we need to choose a dataset. In this assignment we will rely on the [FEVER dataset](https://fever.ai).\n",
        "\n",
        "The dataset is about facts taken from Wikipedia documents that have to be verified. In particular, facts could face manual modifications in order to define fake information or to give different formulations of the same concept.\n",
        "\n",
        "The dataset consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as ```Supported```, ```Refuted``` or ```NotEnoughInfo```. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oa5FpVpT7p4"
      },
      "source": [
        "## 2.1 Dataset structure\n",
        "\n",
        "Relevant data is divided into two file types. Information concerning the fact to verify, its verdict and associated supporting/opposing statements are stored in **.jsonl** format. In particular, each JSON element is a python dictionary with the following relevant fields:\n",
        "\n",
        "*    **ID**: ID associated to the fact to verify.\n",
        "\n",
        "*    **Verifiable**: whether the fact has been verified or not: ```VERIFIABLE``` or ```NOT VERIFIABLE```.\n",
        "    \n",
        "*    **Label**: the final verdict on the fact to verify: ```SUPPORTS```, ```REFUTES``` or ```NOT ENOUGH INFO```.\n",
        "    \n",
        "*    **Claim**: the fact to verify.\n",
        "    \n",
        "*    **Evidence**: a nested list of document IDs along with the sentence ID that is associated to the fact to verify. In particular, each list element is a tuple of four elements: the first two are internal annotator IDs that can be safely ignored; the third term is the document ID (called URL) and the last one is the sentence number (ID) in the pointed document to consider.\n",
        "\n",
        "**Some Examples**\n",
        "\n",
        "---\n",
        "\n",
        "**Verifiable**\n",
        "\n",
        "```\n",
        "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"New_Jersey_Turnpike\", 15]]]}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Not Verifiable**\n",
        "\n",
        "```\n",
        "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nex_8UM4VWuY"
      },
      "source": [
        "## 2.2 Some simplifications and pre-processing\n",
        "\n",
        "We are only interested in verifiable facts. Thus, we can filter out all non-verifiable claims.\n",
        "\n",
        "Additionally, the current dataset format does not contain all necessary information for our classification purposes. In particular, we need to download Wikipedia documents and replace reported evidence IDs with the corresponding text.\n",
        "\n",
        "Don't worry about that! We are providing you the already pre-processed dataset so that you can concentrate on the classification pipeline (pre-processing, model definition, evaluation and training).\n",
        "\n",
        "You can download the zip file containing all set splits (train, validation and test) of the FEVER dataset by clicking on this [link](https://drive.google.com/file/d/1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1/view?usp=sharing). Alternatively, run the below code cell to automatically download it on this notebook.\n",
        "\n",
        "**Note**: each dataset split is in .csv format. Feel free to inspect the whole dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITDtNfau7Fo7"
      },
      "source": [
        "import os, shutil  # file management\n",
        "import sys  # system\n",
        "import pandas as pd  # dataframe management\n",
        "import numpy as np  # data manipulation\n",
        "from tqdm import tqdm  # useful during debugging (progress bars)\n",
        "from typing import List, Callable, Dict  # typing\n",
        "import re  # regex\n",
        "import urllib.request  # download files\n",
        "import zipfile  # unzip files\n",
        "import gensim  # embeddings\n",
        "import gensim.downloader as gloader  # embeddings\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # one-hot encoding\n",
        "from matplotlib import pyplot as plt  # Plots\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  # Remove stopwords\n",
        "from nltk.stem import SnowballStemmer  # Stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.layers import Bidirectional, Dense, SimpleRNN,GlobalAveragePooling1D,Flatten, Concatenate, Add, Average, Dot\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras import Input, Model\n",
        "\n",
        "# F1\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from functools import partial\n",
        "\n",
        "# Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import copy\n",
        "\n",
        "#split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BspxZcRjW0NG"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHntcbb2Gk86"
      },
      "source": [
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "\n",
        "nltk.download('wordnet') \n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "  #delete multiple quotes\n",
        "  delete_multiple_quotes = \"''|``|\\.\\.\"\n",
        "  ris = re.sub(delete_multiple_quotes, '', text)\n",
        "\n",
        "  #get only the sentence, delete the number before it and the keywords after it\n",
        "  start_symbol = \"^[0-9]*\\\\t\"\n",
        "  end_symbol = \"( )?[\\.|\\?|\\!|\\,]( )?(\\\\t.*)?$\"\n",
        "  ris = re.sub(start_symbol, '', ris)\n",
        "  ris = re.sub(end_symbol, '', ris)\n",
        "\n",
        "  #convert the brackets into token, done for the claim string\n",
        "  ris = re.sub(\"\\(\", \" -LRB- \",ris)\n",
        "  ris = re.sub(\"\\)\", \" -RRB- \",ris)\n",
        "\n",
        "  #check if numbers are present between tokens LSB and RSB, if it's not the case delete the content\n",
        "  delete_content_lsb = \"-LRB-(.[^0-9]*)-RRB-\"\n",
        "  ris = re.sub(delete_content_lsb, '', ris)\n",
        "\n",
        "  #check if numbers are present in brackets, if it's not the case delete the content\n",
        "  delete_content_brackets = \"-LSB-(.[^0-9]*)-RSB-\"\n",
        "  ris = re.sub(delete_content_brackets, '', ris)\n",
        "\n",
        "  #delete brackets token\n",
        "  delete_brackets = \"-LRB-|-RRB-|-RSB-|-LSB-\"\n",
        "  ris = re.sub(delete_brackets, ' ', ris)\n",
        "\n",
        "  #deal with the &\n",
        "  ris = re.sub(\"\\&\", ' and ', ris)\n",
        "\n",
        "  #deal with the *\n",
        "  ris = re.sub(\"star * reach\", 'star*reach', ris)\n",
        "\n",
        "  #remove tokens that we are not interested in\n",
        "  remove_tokens = \"[\\-\\\"?!#`\\$]\"  # |[\\.] \" # added $ and \\. handled alone`\n",
        "  ris = re.sub(remove_tokens, ' ', ris)\n",
        "\n",
        "  #delete additional spaces and last space\n",
        "  #remove_spaces = \"[ ]+\"\n",
        "  #remove_last_spaces= \" $\"\n",
        "  #ris = re.sub(remove_spaces, ' ', ris)\n",
        "  #ris = re.sub(remove_last_spaces, '', ris)\n",
        "\n",
        "  #ris = ' '.join([x for x in ris.split() if x and x not in STOPWORDS])\n",
        "  ris.strip()\n",
        "\n",
        "  ris = \" \".join([LEMMATIZER.lemmatize(word) for word in ris.split()])\n",
        "\n",
        "  #handle ' (?)\n",
        "\n",
        "  return ris.lower()\n",
        "\n",
        "id=1294\n",
        "df = pd.read_csv('dataset/test_pairs.csv')\n",
        "df = df.drop(['Unnamed: 0'],axis=1)\n",
        "print(df.iloc[id]['Evidence'])\n",
        "df['Evidence'] = df.apply(lambda row : clean_text(row['Evidence']), axis = 1)\n",
        "df['Claim'] = df.apply(lambda row : clean_text(row['Claim']), axis = 1)\n",
        "print(df.iloc[id]['Evidence'])\n",
        "print(df.iloc[id]['Claim'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-zg38glKHPQ"
      },
      "source": [
        "X = df.drop(['Label','ID'],axis=1)\n",
        "y = df['Label']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.66, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-zg38glKHPQ"
      },
      "source": [
        "X = df.drop(['Label','ID'],axis=1)\n",
        "y = df['Label']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.20, random_state=42)\n",
        "\n",
        "# 1 supports"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbH_8errW5MH"
      },
      "source": [
        "# Classification dataset\n",
        "\n",
        "At this point, you should have a reay-to-go dataset! Note that the dataset format changed as well! In particular, we split the evidence set associated to each claim, in order to build `(claim, evidence)` pairs. The classification label is propagated as well.\n",
        "\n",
        "We'll motivate this decision in the next section!\n",
        "\n",
        "Just for clarity, here's an example of the pre-processed dataset:\n",
        "\n",
        "---\n",
        "\n",
        "**Claim**: \"Wentworth Miller is yet to make his screenwriting debut.\"\n",
        "\n",
        "**Evidence**: \"2\tHe made his screenwriting debut with the 2013 thriller film Stoker .\tStoker\tStoker (film)\"\n",
        "\n",
        "**Label**: Refutes\n",
        "\n",
        "---\n",
        "\n",
        "[**Note**]: The dataset requires some text cleaning as you may have noticed!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH8hIK21Xrl0"
      },
      "source": [
        "# Problem formulation\n",
        "\n",
        "As mentioned at the beginning of the assignment, we are going to formulate the fact checking problem as a binary classification task.\n",
        "\n",
        "In particular, each dataset sample is comprised of:\n",
        "\n",
        "*     A claim to verify\n",
        "*     A set of semantically related statements (evidence set)\n",
        "*     Fact checking label: either evidences support or refute the claim.\n",
        "\n",
        "Handling the evidence set from the point of view of neural models may imply some additional complexity: if the evidence set is comprised of several sentences we might incur in memory problems.\n",
        "\n",
        "To this end, we further simplify the problem by building (claim, evidence) pairs. The fact checking label is propagated as well.\n",
        "\n",
        "Example:\n",
        "\n",
        "     Claim: c1 \n",
        "     Evidence set: [e1, e2, e3]\n",
        "     Label: S (support)\n",
        "\n",
        "--->\n",
        "\n",
        "    (c1, e1, S),\n",
        "    (c1, e2, S),\n",
        "    (c1, e3, S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E46flIz_zQy-"
      },
      "source": [
        "## 4.1 Schema\n",
        "\n",
        "The overall binary classification problem is summed up by the following (simplified) schema\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Wm_YBnFwgJtxcWEBpPbTBEVkpKaL08Jp)\n",
        "\n",
        "Don't worry too much about the **Encoding** block for now. We'll give you some simple guidelines about its definition. For the moment, stick to the binary classification task definition where, in this case, we have 2 inputs: the claim to verify and one of its associated evidences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsiTV-NVdgsF"
      },
      "source": [
        "# Architecture Guidelines\n",
        "\n",
        "There are many neural architectures that follow the above schema. To avoid phenomena like the writer's block, in this section we are going to give you some implementation guidelines.\n",
        "\n",
        "In particular, we would like you to test some implementations so that you explore basic approaches (neural baselines) and use them as building blocks for possible extensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJBQm47fe7iE"
      },
      "source": [
        "## 5.1 Handling multiple inputs\n",
        "\n",
        "The first thing to notice is that we are in a multi-input scenario. In particular, each sample is comprised of a fact and its asssociated evidence statement.\n",
        "\n",
        "Each of these input is encoded as a sequence of tokens. In particular, we will have the following input matrices:\n",
        "\n",
        "*    Claim: `[batch_size, max_tokens]`\n",
        "*    Evidence: `[batch_size, max_tokens]`\n",
        "\n",
        "Moreover, after the embedding layer, we'll have:\n",
        "\n",
        "*    Claim: `[batch_size, max_tokens, embedding_dim]`\n",
        "*    Evidence: `[batch_size, max_tokens, embedding_dim]`\n",
        "\n",
        "But, we would like to have a 2D input to our classifier, since we have to give an answer at pair level. Therefore, for each sample, we would expect the following input shape to our classification block:\n",
        "\n",
        "*   Classification input shape: `[batch_size, dim]`\n",
        "\n",
        "**How to do that?**\n",
        "\n",
        "We inherently need to reduce the token sequence to a single representation. This operation is formally known as **sentence embedding**. Indeed, we are trying to compress the information of a whole sequence into a single embedding vector.\n",
        "\n",
        "Here are some simple solutions that we ask you to try out:\n",
        "\n",
        "1.   Encode token sequences via a RNN and take the last state as the sentence embedding.\n",
        "\n",
        "2.  Encode token sequences via a RNN and average all the output states.\n",
        "\n",
        "3.  Encode token sequences via a simple MLP layer. In particular, if your input is a `[batch_size, max_tokens, embedding_dim]` tensor, the matrix multiplication works on the **max_tokens** dimension, resulting in a `[batch_size, embedding_dim]` 2D matrix. Alternatively, you can reshape the 3D input tensor from `[batch_size, max_tokens, embedding_dim]` to `[batch_size, max_tokens * embedding_dim]` and then apply the MLP layer.\n",
        "\n",
        "4.   Compute the sentence embedding as the mean of its token embeddings (**bag of vectors**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8NEmJp1Ui6z"
      },
      "source": [
        "## Create GloVe embeddings (keep attention to the size, maybe we have to change it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SgHagnSUlxq"
      },
      "source": [
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mff7TRr0V8hv"
      },
      "source": [
        "def check_OOV_terms(embedding_vocabulary: List[str],\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    \n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks3PUZeyFuYB"
      },
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           vocab_size: int,\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def update_embedding_matrix(embedding_model: np.ndarray, \n",
        "                            embedding_dimension: int,\n",
        "                            word_to_idx: Dict[str, int],\n",
        "                            vocab_size: int,\n",
        "                            oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n",
        "\n",
        "    :param embedding_model: pre-trained embedding matrix\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[idx]\n",
        "        except (TypeError, IndexError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPAOAphzoPCY"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHclnVYLXWZN"
      },
      "source": [
        "class KerasTokenizer(object):\n",
        "    \"\"\"\n",
        "    A simple high-level wrapper for the Keras tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
        "                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n",
        "        if build_embedding_matrix:\n",
        "            assert embedding_model_type is not None\n",
        "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
        "\n",
        "        self.build_embedding_matrix = build_embedding_matrix\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.embedding_model_type = embedding_model_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.embedding_matrix = None\n",
        "        self.vocab = None\n",
        "\n",
        "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
        "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
        "\n",
        "        self.tokenizer_args = tokenizer_args\n",
        "\n",
        "    def build_vocab(self, data, **kwargs):\n",
        "        print('Fitting tokenizer...')\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        print('Fit completed!')\n",
        "\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "\n",
        "        if self.build_embedding_matrix:\n",
        "            if self.embedding_model is None:\n",
        "              print('Loading embedding model! It may take a while...')\n",
        "              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n",
        "                                                          embedding_dimension=self.embedding_dimension)\n",
        "            \n",
        "            print('Checking OOV terms in train...')\n",
        "            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n",
        "                                             word_listing=list(self.vocab.keys()))\n",
        "            \n",
        "            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n",
        "\n",
        "            print('Building the embedding matrix for train...')\n",
        "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
        "                                                           word_to_idx=self.vocab,\n",
        "                                                           vocab_size=len(self.vocab)+1,          \n",
        "                                                           embedding_dimension=self.embedding_dimension,\n",
        "                                                           oov_terms=self.oov_terms_train)\n",
        "            print('Done for train!')\n",
        "\n",
        "    def update_vocab(self, data, **kwargs):\n",
        "      self.tokenizer.fit_on_texts(data)\n",
        "      if self.build_embedding_matrix:\n",
        "        old_vocab = self.vocab\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "        print('Checking OOV terms...')\n",
        "        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n",
        "                                         word_listing=list(self.vocab.keys()))\n",
        "        \n",
        "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n",
        "\n",
        "        print('Building the embedding matrix...')\n",
        "        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n",
        "                                                       word_to_idx=self.vocab,\n",
        "                                                       vocab_size=len(self.vocab)+1,          \n",
        "                                                       embedding_dimension=self.embedding_dimension,\n",
        "                                                       oov_terms=self.oov_terms)\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            'build_embedding_matrix': self.build_embedding_matrix,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'embedding_model_type': self.embedding_model_type,\n",
        "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
        "            'embedding_model': self.embedding_model,\n",
        "            'vocab_size': len(self.vocab) + 1,\n",
        "        }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        if type(tokens) == str:\n",
        "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
        "        else:\n",
        "            return self.tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return self.tokenizer.sequences_to_texts(ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGZ4NcIZoVwS"
      },
      "source": [
        "### Downloading embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G9mrvhxYBL3"
      },
      "source": [
        "embedding_dimension = 50\n",
        "embedding_model = load_embedding_model(model_type=\"glove\", \n",
        "                                       embedding_dimension=embedding_dimension)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDzHjGghoYpP"
      },
      "source": [
        "### Creating tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd7WJ8fOerEt"
      },
      "source": [
        "tokenizer_args = {\n",
        "    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n",
        "    'lower' : True,  # default\n",
        "    'filters' : '' \n",
        "}\n",
        "\n",
        "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
        "                           build_embedding_matrix=True,\n",
        "                           embedding_dimension=embedding_dimension,\n",
        "                           embedding_model_type=\"glove\", \n",
        "                           embedding_model=embedding_model)\n",
        "tokenizer.build_vocab(X_train[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_train[\"Claim\"])\n",
        "\n",
        "tokenizer_info = tokenizer.get_info()\n",
        "\n",
        "print('Tokenizer info: ', tokenizer_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA0kdGgcoczP"
      },
      "source": [
        "### Updating tokenizer with validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0jKa0qzNzce"
      },
      "source": [
        "tokenizer.update_vocab(X_val[\"Claim\"])\n",
        "tokenizer.update_vocab(X_test[\"Claim\"])\n",
        "tokenizer.update_vocab(X_val[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_test[\"Evidence\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPHTtHKPkNPx"
      },
      "source": [
        "a = list(tokenizer.vocab.keys())\n",
        "a.sort()\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kxTuGGvotsf"
      },
      "source": [
        "### Padding for x and computation of max sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1uKddQlsCRR"
      },
      "source": [
        "def convert_text(df, tokenizer, is_training=False, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Converts input text sequences using a given tokenizer\n",
        "\n",
        "    :param texts: either a list or numpy ndarray of strings\n",
        "    :tokenizer: an instantiated tokenizer\n",
        "    :is_training: whether input texts are from the training split or not\n",
        "    :max_seq_length: the max token sequence previously computed with\n",
        "    training texts.\n",
        "\n",
        "    :return\n",
        "        text_ids: a nested list on token indices\n",
        "        max_seq_length: the max token sequence previously computed with\n",
        "        training texts.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    text_ids_claim = tokenizer.convert_tokens_to_ids(df['Claim'])\n",
        "    text_ids_evidence = tokenizer.convert_tokens_to_ids(df['Evidence'])\n",
        "\n",
        "    # Padding\n",
        "    if is_training:\n",
        "        max_seq_length_claim = int(np.quantile([len(seq) for seq in text_ids_claim], 0.99))\n",
        "        max_seq_length_evidence = int(np.quantile([len(seq) for seq in text_ids_evidence], 0.99))\n",
        "\n",
        "        if max_seq_length_claim > max_seq_length_evidence:\n",
        "            max_seq_length = max_seq_length_claim\n",
        "        else:\n",
        "          max_seq_length = max_seq_length_evidence\n",
        "\n",
        "    else:\n",
        "        assert max_seq_length is not None\n",
        "\n",
        "    claims = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_claim]\n",
        "    claims = np.array([seq[:max_seq_length] for seq in claims])\n",
        "    \n",
        "    evidences = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_evidence]\n",
        "    evidences = np.array([seq[:max_seq_length] for seq in evidences])\n",
        "\n",
        "    return max_seq_length, np.array([claims, evidences])\n",
        "        \n",
        "\n",
        "max_seq_length, x_train = convert_text(X_train, tokenizer, True)\n",
        "print(\"Max token sequence: {}\".format(max_seq_length))\n",
        "print('X train shape: ', x_train.shape)\n",
        "\n",
        "_, x_val = convert_text(X_val, tokenizer, False, max_seq_length)\n",
        "print('X val shape: ', x_val.shape)\n",
        "\n",
        "_, x_test = convert_text(X_test, tokenizer, False, max_seq_length)\n",
        "print('X test shape: ', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhi2js_i6x0Z"
      },
      "source": [
        "## Sentence embedding\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Ed3CA5FUkM"
      },
      "source": [
        "embedding_vector_length = embedding_dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyF5m7TP5nD7"
      },
      "source": [
        "def firstModel(embedding_vector_length,dim, start_lr=0.001):\n",
        "\n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=True, \n",
        "                      mask_zero=True)(input)\n",
        "  last_state = SimpleRNN(dim,return_state=True)(x)   #can we add a Bidirectional layer??\n",
        "  \n",
        "  RNN = Model(input, last_state, name=\"firstModel\")\n",
        "  return RNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEfIzk7vFNcd"
      },
      "source": [
        "def secondModel(embedding_vector_length,dim, start_lr=0.001):\n",
        "  \n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True)(input)\n",
        "  states = SimpleRNN(dim,return_sequences=True)(x)   #can we add a Bidirectional layer??\n",
        "  output = GlobalAveragePooling1D()(states)\n",
        "  RNN = Model(input, output, name=\"secondModel\")\n",
        "  \n",
        "  \n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  RNN.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "  return RNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7u666kwGsqG"
      },
      "source": [
        "def thirdModel(embedding_vector_length, dim, start_lr=0.001):\n",
        "  \n",
        "  MLP = Sequential()\n",
        "  MLP.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True))\n",
        "  MLP.add(Flatten())\n",
        "  MLP.add(Dense(350, input_shape=(embedding_vector_length*max_seq_length,), activation='relu'))\n",
        "  MLP.add(Dense(50, activation='relu'))\n",
        "  MLP.add(Dense(dim, activation='sigmoid'))\n",
        "  \n",
        "  \n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  MLP.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "\n",
        "  return MLP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSJckeXRHMNs"
      },
      "source": [
        "def fourthModel(embedding_vector_length, start_lr=0.001):\n",
        "  \n",
        "  EMB = Sequential()\n",
        "  EMB.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True))  # do not train embeddings, but we can do it\n",
        "  EMB.add(GlobalAveragePooling1D())\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  EMB.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "  return EMB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gypl5z5ElJo1"
      },
      "source": [
        "## Create GloVe embeddings (keep attention to the size, maybe we have to change it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SgHagnSUlxq"
      },
      "source": [
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mff7TRr0V8hv"
      },
      "source": [
        "def check_OOV_terms(embedding_vocabulary: List[str],\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    \n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks3PUZeyFuYB"
      },
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           vocab_size: int,\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def update_embedding_matrix(embedding_model: np.ndarray, \n",
        "                            embedding_dimension: int,\n",
        "                            word_to_idx: Dict[str, int],\n",
        "                            vocab_size: int,\n",
        "                            oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n",
        "\n",
        "    :param embedding_model: pre-trained embedding matrix\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[idx]\n",
        "        except (TypeError, IndexError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "Qodi5wtE16yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import concatenate, add, average, dot\n",
        "def Classifier(embed_model, model_type, type_merge, cosine_similarity):\n",
        "  input_c = Input(shape=(max_seq_length))\n",
        "  input_e = Input(shape=(max_seq_length))\n",
        "  embedding_c = embed_model(input_c)\n",
        "  embedding_e = embed_model(input_e)\n",
        "\n",
        "  if model_type == \"firstModel\":\n",
        "    embedding_c = embedding_c[1]\n",
        "    embedding_e = embedding_e[1]\n",
        "\n",
        "  if type_merge == \"concat\":\n",
        "      class_input = concatenate([embedding_c, embedding_e])\n",
        "  elif type_merge == \"sum\":\n",
        "      class_input = add([embedding_c, embedding_e])\n",
        "  elif type_merge == \"mean\":\n",
        "      class_input = average([embedding_c, embedding_e])\n",
        "\n",
        "  if cosine_similarity:\n",
        "      cos_sim = dot([embedding_c, embedding_e], axes=1, normalize=True)\n",
        "      class_input = concatenate([class_input, cos_sim])\n",
        "\n",
        "  x = Dense(32, activation=\"relu\")(class_input)\n",
        "  output = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  return Model([input_c, input_e], output, name=\"Classifier\")"
      ],
      "metadata": {
        "id": "fZjsHxzhd4N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.0001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "embed_model = firstModel(embedding_vector_length, 64)\n",
        "base_model = Classifier(embed_model, \"firstModel\", \"mean\", cosine_similarity=True)\n",
        "base_model.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "history = base_model.fit(x=[x_train[0], x_train[1]], y=y_train, \n",
        "                         validation_data=([x_val[0], x_val[1]], y_val), \n",
        "                         epochs=60, batch_size=1024)"
      ],
      "metadata": {
        "id": "GF7mMXyMfeMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = base_model.predict([x_train[0], x_train[1]])\n",
        "print(classification_report(y_train, np.round(predictions)))"
      ],
      "metadata": {
        "id": "3U2xPTPH3HJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = base_model.predict([x_val[0], x_val[1]])\n",
        "print(classification_report(y_val, np.round(predictions)))"
      ],
      "metadata": {
        "id": "j4L2cshu1mYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(tf.keras.Model):\n",
        "  def __init__(self, embedding_vector_length, dim, type_embed, type_merge, \n",
        "               cosine_similarity=False, embed_trainable=True, start_lr=0.001, **kwargs):\n",
        "    super(Classifier, self).__init__(**kwargs)\n",
        "\n",
        "    self.type_embed = type_embed\n",
        "    self.cosine_similarity = cosine_similarity\n",
        "\n",
        "    self.embedding_1 = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                        input_length=max_seq_length, \n",
        "                        trainable=embed_trainable, \n",
        "                        mask_zero=True)\n",
        "    if type_embed == \"firstModel\":\n",
        "      self.embedding_2 = SimpleRNN(dim,return_state=True)\n",
        "    elif type_embed == \"secondModel\":\n",
        "      self.embedding_2 = SimpleRNN(dim,return_sequences=True)\n",
        "      self.embedding_3 = GlobalAveragePooling1D()\n",
        "    elif type_embed == \"thirdModel\":\n",
        "      self.embedding_2 = Flatten()\n",
        "      self.embedding_3 = Dense(350, input_shape=(embedding_vector_length*max_seq_length,), activation='relu')\n",
        "      self.embedding_4 = Dense(50, activation='relu')\n",
        "      self.embedding_5 = Dense(dim, activation='sigmoid')\n",
        "    elif type_embed == \"fourthModel\":\n",
        "      self.embedding_2 = GlobalAveragePooling1D()\n",
        "\n",
        "    if type_merge == \"concat\":\n",
        "      self.merge = Concatenate()\n",
        "    elif type_merge == \"sum\":\n",
        "      self.merge = Add()\n",
        "    elif type_merge == \"mean\":\n",
        "      self.merge = Average()\n",
        "\n",
        "    self.concatenate = Concatenate()\n",
        "\n",
        "    self.cosine_similarity = Dot(axes=1, normalize=True)\n",
        "\n",
        "    self.dense_1 = Dense(16, activation=\"relu\")\n",
        "    self.activation = Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "  def call(self, input):\n",
        "    claim = input[0]\n",
        "    evidence = input[1]\n",
        "    c_embed = self.embedding_1(claim)\n",
        "    e_embed = self.embedding_1(evidence)\n",
        "\n",
        "    c_embed = self.embedding_2(c_embed)\n",
        "    e_embed = self.embedding_2(e_embed)\n",
        "\n",
        "    if self.type_embed == \"thirdModel\":\n",
        "      c_embed = self.embedding_3(c_embed)\n",
        "      e_embed = self.embedding_3(e_embed)\n",
        "      c_embed = self.embedding_4(c_embed)\n",
        "      e_embed = self.embedding_4(e_embed)\n",
        "      c_embed = self.embedding_5(c_embed)\n",
        "      e_embed = self.embedding_5(e_embed)\n",
        "    elif self.type_embed == \"secondModel\":\n",
        "      c_embed = self.embedding_3(c_embed)\n",
        "      e_embed = self.embedding_3(e_embed)\n",
        "\n",
        "    if self.type_embed == \"firstModel\":\n",
        "      c_embed = c_embed[1]\n",
        "      e_embed = e_embed[1]\n",
        "\n",
        "    class_input = self.merge([c_embed, e_embed])\n",
        "\n",
        "    if self.cosine_similarity:\n",
        "      cos_sim = self.cosine_similarity([c_embed, e_embed])\n",
        "      class_input = self.concatenate([class_input, cos_sim])\n",
        "\n",
        "    x = self.dense_1(class_input)\n",
        "    return self.activation(x)"
      ],
      "metadata": {
        "id": "wSg4lr6Rh1EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "AUeEi0Tb6mKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "base_model = Classifier(embedding_vector_length, 32, \"firstModel\", \"mean\", \n",
        "                        embed_trainable=True, cosine_similarity=True)\n",
        "base_model.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "history = base_model.fit(x=[x_train[0], x_train[1]], y=y_train, \n",
        "                         validation_data=([x_val[0], x_val[1]], y_val), \n",
        "                         epochs=100, batch_size=128)"
      ],
      "metadata": {
        "id": "tXnZVgIom7Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvH6c2z00T4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.evaluate([x_test[0], x_test[1]], y_test)"
      ],
      "metadata": {
        "id": "_cQvMPwscFU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPAOAphzoPCY"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHclnVYLXWZN"
      },
      "source": [
        "# Glove -> 50, 100, 200, 300\n",
        "\n",
        "class KerasTokenizer(object):\n",
        "    \"\"\"\n",
        "    A simple high-level wrapper for the Keras tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
        "                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n",
        "        if build_embedding_matrix:\n",
        "            assert embedding_model_type is not None\n",
        "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
        "\n",
        "        self.build_embedding_matrix = build_embedding_matrix\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.embedding_model_type = embedding_model_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.embedding_matrix = None\n",
        "        self.vocab = None\n",
        "\n",
        "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
        "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
        "\n",
        "        self.tokenizer_args = tokenizer_args\n",
        "\n",
        "    def build_vocab(self, data, **kwargs):\n",
        "        print('Fitting tokenizer...')\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        print('Fit completed!')\n",
        "\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "\n",
        "        if self.build_embedding_matrix:\n",
        "            if self.embedding_model is None:\n",
        "              print('Loading embedding model! It may take a while...')\n",
        "              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n",
        "                                                          embedding_dimension=self.embedding_dimension)\n",
        "            \n",
        "            print('Checking OOV terms in train...')\n",
        "            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n",
        "                                             word_listing=list(self.vocab.keys()))\n",
        "            \n",
        "            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n",
        "\n",
        "            print('Building the embedding matrix for train...')\n",
        "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
        "                                                           word_to_idx=self.vocab,\n",
        "                                                           vocab_size=len(self.vocab)+1,          \n",
        "                                                           embedding_dimension=self.embedding_dimension,\n",
        "                                                           oov_terms=self.oov_terms_train)\n",
        "            print('Done for train!')\n",
        "\n",
        "    def update_vocab(self, data, **kwargs):\n",
        "      self.tokenizer.fit_on_texts(data)\n",
        "      if self.build_embedding_matrix:\n",
        "        old_vocab = self.vocab\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "        print('Checking OOV terms...')\n",
        "        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n",
        "                                         word_listing=list(self.vocab.keys()))\n",
        "        \n",
        "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n",
        "\n",
        "        print('Building the embedding matrix...')\n",
        "        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n",
        "                                                       word_to_idx=self.vocab,\n",
        "                                                       vocab_size=len(self.vocab)+1,          \n",
        "                                                       embedding_dimension=self.embedding_dimension,\n",
        "                                                       oov_terms=self.oov_terms)\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            'build_embedding_matrix': self.build_embedding_matrix,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'embedding_model_type': self.embedding_model_type,\n",
        "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
        "            'embedding_model': self.embedding_model,\n",
        "            'vocab_size': len(self.vocab) + 1,\n",
        "        }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        if type(tokens) == str:\n",
        "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
        "        else:\n",
        "            return self.tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return self.tokenizer.sequences_to_texts(ids)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGZ4NcIZoVwS"
      },
      "source": [
        "### Downloading embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G9mrvhxYBL3",
        "outputId": "d20abfc0-b3bf-4a7f-a8de-0f763b16ec88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding_dimension = 50\n",
        "embedding_model = load_embedding_model(model_type=\"glove\", \n",
        "                                       embedding_dimension=embedding_dimension)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDzHjGghoYpP"
      },
      "source": [
        "### Creating tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd7WJ8fOerEt",
        "outputId": "73cacd93-2b34-4ff0-ebff-dee175943b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer_args = {\n",
        "    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n",
        "    'lower' : True,  # default\n",
        "    'filters' : '' \n",
        "}\n",
        "\n",
        "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
        "                           build_embedding_matrix=True,\n",
        "                           embedding_dimension=embedding_dimension,\n",
        "                           embedding_model_type=\"glove\", \n",
        "                           embedding_model=embedding_model)\n",
        "tokenizer.build_vocab(X_train[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_train[\"Claim\"])\n",
        "\n",
        "tokenizer_info = tokenizer.get_info()\n",
        "\n",
        "print('Tokenizer info: ', tokenizer_info)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting tokenizer...\n",
            "Fit completed!\n",
            "Checking OOV terms in train...\n",
            "Total OOV terms: 166 (2.44%)\n",
            "Building the embedding matrix for train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 6816/6816 [00:00<00:00, 240707.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done for train!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking OOV terms...\n",
            "Total OOV terms: 1189 (14.85%)\n",
            "Building the embedding matrix...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8005/8005 [00:00<00:00, 409211.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer info:  {'build_embedding_matrix': True, 'embedding_dimension': 50, 'embedding_model_type': 'glove', 'embedding_matrix': (8006, 50), 'embedding_model': <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f0e40036d50>, 'vocab_size': 8006}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA0kdGgcoczP"
      },
      "source": [
        "### Updating tokenizer with validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0jKa0qzNzce",
        "outputId": "741d0dfa-3652-4ec9-d795-8cf8c02ab0ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.update_vocab(X_val[\"Claim\"])\n",
        "tokenizer.update_vocab(X_test[\"Claim\"])\n",
        "tokenizer.update_vocab(X_val[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_test[\"Evidence\"])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking OOV terms...\n",
            "Total OOV terms: 582 (6.78%)\n",
            "Building the embedding matrix...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8587/8587 [00:00<00:00, 449908.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking OOV terms...\n",
            "Total OOV terms: 98 (1.13%)\n",
            "Building the embedding matrix...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8685/8685 [00:00<00:00, 699547.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking OOV terms...\n",
            "Total OOV terms: 906 (9.45%)\n",
            "Building the embedding matrix...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9591/9591 [00:00<00:00, 414379.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking OOV terms...\n",
            "Total OOV terms: 153 (1.57%)\n",
            "Building the embedding matrix...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9744/9744 [00:00<00:00, 675056.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPHTtHKPkNPx",
        "outputId": "7a8527e3-f4b0-4af9-da2a-b924fa36fb84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = list(tokenizer.vocab.keys())\n",
        "a.sort()\n",
        "print(a)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['%', \"'\", \"'71\", \"'m\", \"'re\", \"'s\", '*', ',', '.', '/', '0', '0.9', '02', '07', '09', '1', '1,', '1,000', '1,441', '1,447', '1,500', '1.05', '1.2', '1.29', '1.3', '1.381', '1.5', '1.6', '1.8', '1.98', '10', '10,', '10,000', '10,000,000', '100', '100,000', '1000', '1000,', '101st', '103,219', '103,985', '104', '104th', '10th', '11', '11%', '11,', '11.3', '115', '117,400', '11:00', '11th', '11th,', '12', '12,', '12.3', '120', '1203', '1207', '120th', '1216', '1234', '1246', '126', '1268', '1272', '1297', '12th', '12th,', '13', '13,', '130', '1336', '1370', '139', '1397', '1399', '13th', '13th,', '14', '14,', '14.3', '1405', '143', '1430', '1450', '146', '1466', '1470', '1471', '148', '1480', '1483', '148326', '1486', '1491', '1492', '1493', '1499', '14th', '14th,', '15', '15,', '15,882,417', '15,900', '15.4', '150', '1502', '1503', '1509', '150th', '1522', '1524', '1530', '1533', '1545', '1547', '1549', '155', '1553', '1555', '1558', '1576', '1589', '1596', '15th,', '16', '16,', '160', '1600', '1600s', '1603', '1607', '1617', '1618', '1627', '163', '1635', '1636', '1654', '1685', '16:21', '16million', '16th', '17', '17,', '170', \"1700's\", '1700s', '1717', '1718', '173', '1730', '1738', '1740', '1744', '1751', '1756', '1757', '1758', '1762', '1764', '1765', '1769', '1777', '1783', '1785', '1788', '1788,', '1790', '1791', '1794', '1799', '17th', '17th,', '18', '18,', \"1800's\", '1800s', '1805', '1806', '1808', '1809', '1810', '1811', '1818', '182', '1820', '1830', '1831', '1845', '1847', '1850s', '1853', '1854', '1855', '1856', '1857', '1858', '1859', '1860', '1860,', '1861', '1862', '1864', '1865', '1869', '1870', '1872', '1874', '1875,', '1878', '1880', '1884', '1885', '1885,', '1888', '1889', '1890', '1891', '1893', '1894', '1895', '1897', '18th', '18th,', '19', '19,', '1900', '1900,', '1900s', '1901', '1902', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1910s', '1911', '1912', '1913', '1914', '1915', '1916', '1918', '1919', '1920', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', \"1930's\", '1931', '1932', '1933', '1934', '1935', '1936', '1936,', '193624', '1937', '1938', '1940', '1940s', '1941', '1942', '1943', '1944', '1945', '1945,', '1946', '1947', '1949', '1950', \"1950's\", '1950s', '1951', '1952', '1954', '1955', '1956', '1957', '1958', '1959', '1960', \"1960's\", \"1960's,\", '1960s', '1961', '1962', '1963', '1963,', '1964', '1965', '1966', '1966,', '1967', '1968', '1969', '1970', \"1970's\", \"1970's,\", '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1977,', '1978', '1979', '1980', \"1980's\", '1980s', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1987\\tbritish', '1988', '1989', '1990', '1990,', '1990s', '1991', '1992', '1992,', '1993', '1994', '1995', '1996', \"1996's\", '1997', '1998', '1998,', '1999', '19th', '19th,', '1st', '1st,', '2', '2,', '2,525', '2,808,356', '2,925', '2.5', '2.6', '2.66', '2.85', '20', '20,', '20,300,000', '20.3', '200', '2000', '2000s', '2001', '2002', '2003', '2004', '2004,', '2005', '2005,', '2006', '2006,', '2007', '2007,', '2008', '2008,', '2009', '2009,', '201', '2010', '2010s', '2011', '2011,', '2012', '2012,', '2013', '2014', '2014,', '2015', '2015,', '2016', '2016,', '2017', '2018', '202', '2023', '203,850', '206', '209', '20th', '20th,', '21', '21,', '218', '219', '21st', '22', '22,', '221', '22nd,', '23', '23,', '235', '238', '23rd', '23rd,', '24', '24,', '24th', '24th,', '25', '25,', '250', '250,750', '253', '255', '25th', '26', '26,', '26,000', '26th,', '27', '27,', '27th', '28', '28,', '28th', '28th,', '29', '29,500', '2925', '29th', '2d', '2nd', '2nd,', '3', '3,', '3,090,953', '3,363', '3.8', '30', '30%', '30,', '300', '304', '31', '31,', '31.9', '32', '32,000', '320', '324', '32nd', '33', '330', '338.6', '34', '343', '348', '35', '355', '36', '36,000', '36th', '37', '37.01', '37th', '38', '39', '3d', '3rd', '3rd,', '4', '4,', '4,114', '4,600', '4.08', '4.5', '4.7', '40', '40th', '41', '417', '41st', '42', '42nd', '430', '43rd', '44%', '440', '45', '46', '46th', '47.9', '470', '472,522', '479', '497', '49ers', '4th', '4th,', '5', '5%', '5,', '5,000', '5.5', '5.7', '50', '50%', '50,000', '500', '500,000', '50000', '506,615', '50th', '51st', '520,000', '527,970', '53', '5387', '53879', '54', '55', '553', '554', '554.5', '555', '56', '560', '57', '575', '575,000', '57th', '589', '59', '591', '59th', '5th', '5th,', '6', '6,', '6,852', '6.2', '6.3', '60,000', '60.2', '600,000', '604', '61', '62,000', '62,000,000', '62,527', '64', '65', '65th', '66th', '68', '681', '687', '689', '69,723', '6th', '7', '7,', '7,500,000,000', '7.66', '7.68', '7.7', '70', '70%', '70,231', '70,300,000', '70s', '71', '711', '712', '74', '75', '756', '76', '767', '77', '78', '787', '79.39', '7e7', '8', '8,', '8.5', '8.7', '80', '80%', '800,000', '83.3', '83.3%', '83rd', '85', '85.93', '86', '87', '87,894', '879', '88', '89', '8th', '9', '9,', '9,596', '9.8', '9.x', '90', \"90's\", '92', '93', '95', '953', '96,931,000', '97', '972', '99', '992', '9th', '9th,', ':', ';', '=', 'OOV_TOKEN', 'a', 'a.', 'a.j.', 'a.p.j.', 'a380', 'a4', 'aaj', 'aaron', 'aba', 'abandoned', 'abbreviated', 'abc', 'abdel', 'abdul', 'abernathy', 'abilities', 'ability', 'able', 'abnormal', 'aboard', 'about', 'above', 'abraham', 'abrahamic', 'abrams', 'abroad', 'absinthe', 'absinthium', 'absolute', 'absorbed', 'absorbs', 'absurdist', 'abuse', 'abuser', 'abv', 'academic', 'academics', 'academy', 'academy,', 'accepted', 'access', 'accessed', 'accessing', 'accessories', 'acche', 'accident', 'accidental', 'acclaim', 'acclaimed', 'accolades', 'accompanied', 'accomplishment', 'according', 'account', 'accountant', 'accounts', 'accused', 'acetylcholine', 'achieve', 'achieved', 'achievement', 'acid', 'acknowledged', 'acoustic', 'acquired', 'across', 'act', 'acted', 'acting', 'action', 'action/animated', 'action/computer', 'actions,', 'activating', 'active', 'actively', 'activewear', 'activism', 'activist', \"activist's\", 'activists', 'activities', 'activity', 'actor', 'actor,', 'actors', 'actress', \"actress's\", 'actresses', 'acts', 'actual', 'actually', 'ad', 'adam', 'adams', 'adaptation', 'adaptations', 'adapted', 'added', 'addiction', 'addison', 'addition', 'additional', 'additionally', 'address', 'aden', 'adequate', 'adhere', 'adherents', 'adidas', 'aditya', 'adjacent', 'adjunct', 'administration', 'administrative', 'admiral', 'admission', 'adobe', 'adolf', 'adopted', 'adopting', 'adoption', 'adornments', 'adrianne', 'adult', 'adults', 'advanced', 'advancements', 'advances', 'advent', 'adventure', 'adventure,', 'adventures', 'advisor', 'advocate', 'advocates', 'ae', 'aegean', 'aeneas', 'aerial', 'aeronautical', 'aesthetic', 'aestheticism', 'aesthetics', 'afc', 'afcon', 'affairs', 'affect', 'affected', 'affiliation', 'affleck', 'afflicted', 'afghan', 'afghanistan', 'africa', 'africa,', 'african', 'after', 'ag', 'again', 'against', 'agamic', 'age', 'aged', 'agencies', 'agency', 'agent', 'agents', 'ages', 'aggregating', 'aging', 'agnes', 'ago', 'agree', 'agreed', 'agreement', 'agricultural', 'agriculture', 'aguilera', 'ahmed', 'aided', 'ailments', 'aim', 'aimed', 'aims', 'air', 'airbender', 'airborne', 'airbus', 'aircraft', 'aired', 'aires', 'airing', 'airline', 'airliner', 'airliners', 'airlines', 'airlines,', 'airplanes', 'airplay', 'airport', 'airports', 'airs', 'airspeed', 'airways', 'ajaccio', 'akbar', 'akon', 'akshay', 'al', 'alabama', 'alabama,', 'alamos', 'alan', 'alaska', 'albert', 'alberto', 'album', 'album,', 'albums', 'alcindor', 'alcohol', 'alcoholic', 'aldous', 'alec', 'alecia', 'alex', 'alexander', 'alexandria', 'alfred', 'algae', 'algeria', 'algonquian', 'ali', 'aliaume', 'alice', 'alien', 'aliens', 'aligned', 'alive', 'alive.', 'all', 'allard', 'alleged', 'allegiant', 'allen', 'allende', 'alliance', 'allow', 'allowed', 'allowing', 'ally', 'almanac', 'almost', 'alone', 'along', 'alongside', 'alonso', 'alpes', 'alpine', 'also', 'alt', 'alternate', 'alternative', 'although', 'altman', 'altogether', 'aluminosilicate', 'alumni', 'alvin', 'always', 'amadeus', 'amancio', 'amanda', 'amateur', 'amazon', 'ambassador', 'ambassadors', 'ambedkar', 'amber', 'ambient', 'ambition', 'ambrosia', 'ambush', 'amc', 'amelia', 'america', \"america's\", 'america,', 'american', 'american.', 'americans', 'americans,', 'americas', 'amharic', 'amir', 'amitabh', 'amma', 'amnesia', 'amoebae', 'among', 'amonute', 'amount', 'amy', 'an', 'analog', 'analogous', 'anarchist', 'anatolian', 'anatomy', 'ancestors', 'ancient', 'and', 'and/or', 'anderson', 'andes', 'andorra', 'andre', 'andrea', 'andrew', 'android', 'andromeda', 'andr', 'andy', 'ang', 'angel', 'angela', 'angeles', 'angeles,', 'angels', 'anger', 'anglian', 'anglican', 'anglo', 'angoulme', 'angus', 'animal', 'animals', 'animated', 'animation', 'animator', 'animators', 'anise', 'anjali', 'ann', 'anna', 'annabelle', 'annapolis', 'annapolis,', 'anne', 'anneliese', 'annelise', 'annexed', 'annie', 'anniversary', 'announced', 'announcing', 'annual', 'annually', 'annul', 'another', 'anse', 'antagonist', 'antarctica', 'antelope', 'antenna', 'anterograde', 'anthology', 'anthony', 'anthropological', 'anti', 'antibiotic', 'antichrist', 'anticipated', 'antigua', 'antilles', 'antiquity', 'antoine', 'anton', 'antonio', 'antony', 'anwar', 'anxiety', 'anxious', 'any', 'anyone', 'anything', 'anywhere', 'aol', 'ap', 'apamea', 'apart', 'apex', 'aphrodite', 'apocalypse', 'apolitical', 'apollo', 'apostle', 'apostles', 'apostolic', 'apparent', 'apparently', 'appeal', 'appear', 'appearance', 'appearances', 'appeared', 'appearing', 'appears', 'appetite', 'apple', 'apples', 'appliances', 'application', 'applications', 'applied', 'appointed', 'appointee', 'appointment', 'appomattox', 'apprenticeship', 'approval', 'approved', 'approximately', 'april', 'aquitaine', 'arab', 'arabia', 'arabian', 'arabic', 'arabs', 'arachnids', 'aragon', 'aramaic', 'aramis', 'arc', 'archaic', 'archdiocese', 'archipelago', 'architect', 'architects', 'arctic', 'are', 'area', \"area's\", 'areas', 'arena', 'arenas', 'ares', 'argentina', \"argentina's\", 'argentine', 'argentinian', 'argo', 'arianism', 'ariel', 'arijit', 'arima', 'aristocracy', 'aristokratia', 'arizona', 'arizona,', 'arjit', 'arlington', 'arlington,', 'arm', 'armed', 'armenian', 'armies', 'arms', 'army', 'arnold', 'aroni', 'arose', 'around', 'arquette', 'arranged', 'arrangement', 'array', 'arrested', 'arrival', 'arrived', 'arriving', 'arryn', 'art', 'art,', 'arte', 'artemisia', 'arthropod', 'arthur', 'article', 'articles', 'artisans', 'artist', 'artistic', 'artists', 'arts', 'arts,', 'artwork', 'aruba', 'as', 'asante', 'asap', 'asia', 'asian', 'asked', 'asking', 'asleep', 'aspects', 'aspirations', 'aspire', 'aspired', 'assassinated', 'assassination', 'assault', 'assembled', 'assent', 'assessing', 'assist', 'assistant', 'assists', 'associate', 'associated', 'association', 'assume', 'assumed', 'assyrian', 'asteroids', 'astronaut', 'astronomical', 'astronomy', 'at', 'athlete', 'athletic', 'athletics', 'athos', 'atlanta', 'atlantic', 'atmosphere', 'ato', 'atomic', 'atp', 'attached', 'attack', 'attacked', 'attacks', 'attempt', 'attempts', 'attenborough', 'attend', 'attendance', 'attended', 'attention', 'attested', 'attitudes', 'attorney', 'attracting', 'attraction', 'attractions', 'attracts', 'attributed', 'attributes', 'aubrey', 'audience', 'audiences', 'audio', 'auditorium', 'audrey', 'august', 'august,', 'augustus', 'augutus', 'aunt', 'austin', 'australia', 'australian', 'austria', 'austrian', 'auteur', 'author', 'authored', 'authoritative', 'authorities', 'authority', 'authorized', 'authors', \"authors'\", 'autism', 'autistic', 'autobiographical', 'autobiography', 'automated', 'automatic', 'automobile', 'automotive', 'autonomous', 'auvergne', 'available', 'avatar', 'avenger', 'avengers', 'average', 'averaged', 'avert', 'avery', 'aviles', 'avoid', 'avoided', 'avoids', 'avril', 'avul', 'awadhi', 'awake', 'awakening', 'awakens', 'award', 'awarded', 'awards', 'awareness', 'away', 'awkward', 'ayan', 'ayananka', 'ayatollah', 'azam', 'azithromycin', 'b', 'b.', 'baadshah', \"baadshah's\", 'baal', 'baba', 'babbit', 'babe', 'babe,', 'baboon', 'babur', 'baby', 'bachchan', 'bachman', 'bachna', 'bachrach', 'back', 'backed', 'background', 'backing', 'backlash', 'bacon', 'bacterial', 'bad', 'badara', 'bade', 'baelish', 'bafta', 'bagarozzi', \"baha'i\", 'bahamas', 'bahu', 'baja', 'bajirao', 'baker', 'bakersfield', 'bakuman', 'bala', 'balaji', 'balance', 'baldwin', 'balibo', 'balkan', 'balkans', 'ball', 'ballad', 'ballads', 'balliol', 'balloons', 'balls', 'baloch', 'balochi', 'balochistan', 'baltic', 'baluch', 'bana', 'banadinovi', 'band', 'banderas', 'bandicoot', 'bandits', 'banerjee', 'banger', 'bangladesh', 'bangor,', 'bank', 'bankable', 'banker', 'banking', 'banks', 'banned', 'banner', 'banque', 'banquet', 'bans', 'baptised', 'bar', 'barack', 'baratheon', 'barbadian', 'barbados', 'barbara', 'barbarella', 'barber', 'barbuda', 'barcelona', 'barclays', 'barden', 'bare', 'barely', 'bargain', 'barks', 'barlas', 'barlow', 'barnacles', 'barnes', 'barons', \"barons'\", 'barred', 'barry', 'barthes', 'barton', 'barty', 'baryonic', 'base', 'baseball', 'based', 'basic', 'basis', 'basketball', 'bastards', \"bastards'\", 'bastia', 'bateman', 'bateman,', 'bates', 'batman', 'battalion', 'battle', 'battled', 'battling', 'batwoman', 'bauer', 'bavaria', 'bavarian', 'bay', 'bayern', 'bbc', 'bc', 'bc,', 'bce', 'bce,', 'be', 'beach', 'beak', 'bear', 'beat', 'beats', 'beatty', 'beaufoy', 'beautiful', 'beauty', 'became', 'because', 'beckingham', 'beckner', 'become', 'becomes', 'becoming', 'bedfordshire', 'bee', 'been', 'beethoven', 'before', 'began', 'begin', 'beginning', 'begins', 'beguiled', \"beguiled's\", 'begun', 'behave', 'behaves', 'behavior', 'beheaded', 'behind', 'beijing', 'being', 'beings', 'belgian', 'belgium', 'belgrade', 'belichick', 'belief', 'beliefs', 'believe', 'believed', 'believers', 'believing', 'bellas', 'belle', 'bells', 'bellwether', 'belong', 'belonged', 'belonging', 'belongs', 'below', 'ben', 'bender', 'beneckendorff', 'benedict', 'benefiting', 'benefits', 'bengal', 'bengali', 'benioff', 'benjamin', 'bennett', 'benny', 'benoit', \"benoit's\", 'benz', 'benzene', 'benzodiazepine', 'benzodiazepines', 'benzos', 'beowulf', 'bequeathing', 'berber', 'berbers', 'bercow', 'berg', 'bering', 'berlin', 'bermuda', 'bernard', 'bernd', 'bernice', 'bernthal', 'berry', 'bertram', 'bes', 'besides', 'best', 'bestselling', 'beth', 'bethesda', 'betsy', 'better', 'betty', 'between', 'beverage', 'beverly', 'beyond', 'bezopasnosti', 'bhagya', 'bhai', 'bharat', 'bharatiya', 'bhayandar', 'bhi', 'bhimrao', 'bhojpuri', 'bhutan', 'bianca', 'bible', 'biblically', 'bid', 'biderman', 'biennial', 'biennially', 'big', 'big.', 'bigger', 'biggest', 'bihari', 'bilingual', 'bill', 'billboard', 'bille', 'billed', 'billie', 'billion', 'billionaire', 'billy', 'binding', 'binoche', 'binomen', 'biochemical', 'biographical', 'biographies', 'biological', 'biologist', 'biology', 'biomedical', 'biopic', 'bird', 'birds', 'birmingham', 'birth', 'birthday', 'birthed', 'birthplace', 'bishop', 'bismarck', 'bitter', 'biz', 'bjrk', 'black', 'blacksmiths', 'blackstone', 'blake', 'blanchett', 'blanco', 'blank', 'blind', 'blinded', 'blink', 'bloch', 'block', 'blockbuster', 'blocks', 'blog', 'blomkamp', 'blood', 'blown', 'blu', 'blue', 'blues', 'bluestone', 'blunt', 'bly', 'bmw', 'board', 'boardgame', 'boat', 'boats', 'bob', 'bodies', 'body', 'body,', 'bodyguard', 'boeing', 'bohemia', 'boise', 'boleyn', 'bolivia', 'bolivia\\tlist', 'bollywood', 'bolton,', 'bombay', 'bombay,', 'bombs', 'bomer', \"bomer's\", 'bonaire', 'bond', 'bones', 'bono', 'bono,', 'bonobo', 'bonobos', \"bonobos'\", 'bonus', 'book', 'booker', 'books', 'booth', 'border', 'bordered', 'bordering', 'borders', 'born', 'borough', 'bose', 'boseman', 'bosh', 'bosnian', 'boston', 'boston,', 'bosworth', 'botanically', 'botanicals', 'both', 'bottom', 'bought', 'boulevard', 'bouncer', 'boundaries', 'bounty', 'bounty,', 'bourbon', 'bourbons', 'bourne', 'bow', 'bowen', \"bowen's\", 'bowl', 'bowls', 'box', 'boxer', 'boxers', 'boxes', 'boxing', 'boy', 'boyd', 'boyle', 'boys', 'bracelet', 'brad', 'brady', 'brahmi', 'braided', 'brain', 'bran', 'branch', 'branches', 'brand', 'branded', 'branding', 'brando', 'brands', 'brandt', 'brasier', 'brat', 'braunstein', 'brazil', 'brazilian', 'breaching', 'break', 'breakaway', 'breakfast', 'breaks', 'breakthrough', 'breath', 'breathing', 'breed', 'brent', 'brenton', 'brett', 'brian', 'brianne', 'brick', 'brickyard', 'bride', 'bridge', 'bridges', 'bridget', 'brie', 'brief', 'briefly', 'bringing', 'brings', 'brit', 'britain', 'britain,', 'british', 'britney', 'briton', 'britons', 'brittany', 'broad', 'broadcast', 'broadcaster', 'broadcasting', 'broader', 'broadly', 'broadsheet', 'broadway', 'broke', 'brokeback', 'broken', 'brolin', 'bront', 'brooklyn', 'brooks', 'bros', 'bros.', 'brother', 'brothers', 'brought', 'brown', 'brownish', 'browser', 'browsers', 'browsing', 'bruce', 'bruckheimer', 'brunswick', 'bryan', 'bryant', 'buck', 'buckingham', 'bucks', 'buddhist', 'buddies', 'budget', 'buena', 'buenos', 'buffy', 'build', 'building', 'buildings', 'buildup', 'built', 'bulgaria', 'bulgarian', 'bull', 'bulldog', 'bulls', 'bumpass', 'burbank', 'burbank,', 'burden', 'bureau', 'burgess', 'burials', 'buried', 'burj', 'burl', 'burton', 'buscana', 'buses', 'bush', \"bush's\", 'busiest', 'business', 'businesses', 'businessman', 'businesswoman', 'but', 'butcher', 'butler', 'butte,', 'butterflies', 'butterfly', 'by', 'byrne', 'byron', 'byzantines', 'c', 'c.k', 'c.k.', 'ca.', 'cable', 'caesar', 'caesars', 'cagefighting', 'cagney', 'caicos', 'cairo', 'calculated', 'calculations', 'calculus', 'caleb', 'calendar', 'california', 'california,', 'californian', 'caligula', 'call', 'called', 'calls', 'cambridge', 'cambridgeshire', \"cambridgeshire's\", 'came', 'cameo', 'cameos', 'camera', 'cameron', 'camilla', 'camp', 'campaign', 'campaigned', 'campaigning', 'campaigns', 'campbell', 'campus', 'campuses', 'can', 'canada', 'canadian', 'cancer', 'cancers', 'candidate', 'candy', 'cannes', 'cannon', 'cannot', 'canon', 'canyon', 'canyons', 'cap', 'capable', 'capcom', 'cape', 'capetian', 'capital', 'capitol', 'capote', \"capote's\", 'cappella', 'capra', 'captain', 'captivity', 'capture', 'captured', 'car', 'carcaterra', 'cardinal', 'career', 'carey', \"carey's\", 'cargo', 'caribbean', \"caribbean's\", 'carl', 'carlos', 'carly', 'carnegie', 'carnival', 'carnivle', 'carol', 'carolina', 'carolina,', 'carpenter', \"carpenter's\", 'carrey', 'carrie', 'carroll', 'carry', 'carrying', 'cars', 'cartel', 'carter', 'cartoon', 'cartridge', 'cary', 'casa', 'cascade', 'cascades', 'cases', 'casey', 'cash', 'casino', 'casinos', 'cassel', 'cassette', 'cassidy', 'cast', 'castile', 'casting', 'castle', 'castlevania', 'casualties', 'cat', 'catalonia', 'catalonian', 'catcher', 'catching', 'cate', 'categories', 'categorized', 'category', 'catering', 'cathedral', 'catherine', 'catholic', 'catholicism', 'cats', 'cattle', 'caucasus', 'caucus', 'cause', 'caused', 'causes', 'cavanagh', 'caviezel', 'cavill', 'cbc', 'cbe', 'cbs', 'cc', 'cd', 'cdre', 'ce', 'cede', 'celebrated', 'celebrating', 'celebration', 'celebrities', 'celebrity', 'celestial', 'cells', 'cellular', 'censorship', 'census', 'census,', 'centaurs', 'center', 'centered', 'centers', 'central', 'centralized', 'centre', 'centuries', 'century', 'ceo', 'cera', 'cereal', 'ceremony', 'ceres', 'certain', 'certification', 'certified', 'chabon', 'chadwick', 'chagatai', 'chaguanas', 'chaguaramas', 'chain', 'chair', 'chairman', 'chairs', 'chalayan', 'challenge', 'challenge:', 'challenged', 'challenger', 'champion', 'champions', 'championship', 'champs', 'chandler', 'change', 'changed', 'changes', 'changing', 'channel', 'chaperone', 'chaplin', 'chappie', 'chappie,', 'character', 'characteristics', 'characterization', 'characterized', 'characters', 'charan', 'charge', 'charged', 'charity', 'charles', 'charlie', 'charlotte', 'charlotte,', 'charlton', 'chart', 'charted', 'charterhouse', 'charts', 'chase', 'chashmah', 'chastity', 'chastity,', 'chat', 'checked', 'cheerleader', 'cheese', 'chef', 'chekhov', 'chelmsford', 'chelsea', 'chelsey', 'chemical', 'chemically', 'chemicals', 'chemistry', 'chennai', 'cher', 'cher,', 'chesley', 'chesney', \"chesney's\", 'chest', 'chevy', 'chicago', 'chicago,', 'chief', 'chien', 'child', 'childhood', 'children', 'chile', 'chili', 'chillwave', 'chilopods', 'chimpanzee', \"chimpanzee's\", 'china', 'chinatown', \"chinatown's\", 'chinese', 'chip', 'chipmunks', \"chipmunks's\", 'chips', 'chiranjeevi', 'chlamydia', 'choice', 'choir', 'choose', 'chopra', 'choreographer', 'chosen', 'chowder', 'chris', 'christ', 'christian', 'christianity', 'christians', 'christina', 'christine', 'christmas', 'christopher', 'chronicle', 'chrysostomus', 'chuck', 'chumlee', 'church', 'churches', 'cia', 'cie', 'cigarette', 'cinema', 'cinemas', 'cinematic', 'cinematographer', 'cinematography', 'cinergi', 'circle', 'circulating', 'circumstances', 'cisalpine', 'cites', 'cities', 'citigroup', 'citizen', 'citizens', 'citizenship', 'city', 'city,', 'civic', 'civil', 'civilian', 'civilizations', 'cj', 'clade', 'claim', 'claimed', 'claiming', 'claims', 'claire', 'clan', 'clancy', 'clara', 'clarence', 'clark', 'clarke', 'clash', 'class', 'classes', 'classic', 'classical', 'classics', 'classifications', 'classified', 'claude', 'claudian', 'claudio', 'claudius', 'clause', 'cleaner', 'cleared', 'clearing', 'cleveland', 'client', 'climate', 'clint', 'clinton', 'clippers', 'clooney', 'cloris', 'close', 'closed', 'closer', 'closeted', 'closing', 'clothes', 'clothing', 'cloud', 'cloverfield', 'clown', 'club', 'clubs', 'clueless', 'clutch', 'clment', 'cmde', 'cmdre', 'cns', 'co', 'coach', 'coaching', 'coalition', 'coast', 'coastal', 'coaster', 'coasts', 'cochran', 'code', 'codes', 'cody', 'coeducational', 'coefficients', 'coeliac', 'coen', 'coeur,', 'coexists', 'cohen', 'cohn', 'coinciding', 'coins', 'colbert', 'colchian', 'cold', 'colin', 'collaborated', 'collaboration', 'collaborations', 'collaborator', 'collapse', 'collapsed', 'collecting', 'collection', 'collective', 'collectively', 'collectivity', 'college', 'collins', 'colombia', 'colombian', 'colonial', 'colonies', 'colonists', 'colony', 'color', 'coloring', 'colosseum', 'colson', 'columbia', 'columbus', 'coma', 'combat', 'combatants', 'combine', 'combined', 'combines', 'come', 'comedian', 'comedic', 'comedienne', 'comedies', 'comedies,', 'comedy', 'comes', 'comets', 'comic', 'comics', 'coming', 'command', 'commander', 'commanding', 'commandment', 'commedia', 'commenced', 'commentator', 'commerce', 'commercial', 'commission', 'commit', 'committed', 'committee', 'committees', 'commodore', 'common', 'commonly', 'commonplace', 'commons', 'commonwealth', 'commune', 'communications', 'communicator', 'communist', 'communities', 'community', 'commuter', 'commuters', 'como', 'compact', 'companies', 'companion', 'companionship', 'company', 'comparison', 'compete', 'competed', 'competes', 'competing', 'competition', 'competitive', 'competitively', 'competitiveness', 'compilation', 'compiled', 'complaints', 'complementary', 'complete', 'completed', 'completely', 'completing', 'complex', 'complications', 'component', 'components', 'composed', 'composer', 'composition', 'compound', 'comprised', 'comprises', 'compton', 'computer', 'computers', 'comune', 'conceived', 'concentration', 'concept', 'concepts', 'concerned', 'concerns', 'concert', 'concerts', 'concluded', 'concrete', 'condell', 'condemned', 'condition', 'conditions', 'confederacy', 'confederate', 'confederates', 'confederation', 'conference', 'conferred', 'confessed', 'confined', 'confirmed', 'conflict', 'conflicts', 'confluence', 'conglomerate', 'congo', 'congress', 'congressional', 'connect', 'connected', 'connecticut', 'connection', 'connections', 'connery', 'conquered', 'conqueror', 'conrad', 'conscious', 'conscription', 'conscripts', 'consecutive', 'consensus', 'conservation', 'conservatism', 'conservative', 'conservatives', 'considerable', 'considerably', 'considered', 'consist', 'consisted', 'consisting', 'consists', 'console', 'consolidated', 'consonant', 'consort', 'conspiracy', 'constanten', 'constantine', 'constanze', 'constanze,', 'constituent', 'constitute', 'constitution', 'constitutional', 'constructed', 'construction', 'consultant', 'consumed', 'consumer', 'contador', 'contain', 'contained', 'contains', 'contemporary', 'content', 'contestant', 'contestants', 'contested', 'contexts', 'contiguous', 'continent', 'continental', 'continents', 'contingent', 'continue', 'continued', 'continues', 'continuously', 'contract', 'contracted', 'contraction', 'contracts', 'contrary', 'contrasting', 'contribute', 'contributed', 'contributing', 'contribution', 'control', 'controlled', 'controller', 'controlling', 'controls', 'controversial', 'controversy', 'conurbation', 'convention', 'conventional', 'conversion', 'conversions', 'converted', 'converting', 'convict', 'convicted', 'cook', 'cooking', 'cooling', 'cooper', 'coordinates', 'coordinates,', 'copies', 'coproduction', 'corden', 'cordilleran', 'core', 'cornell', 'corner', 'corp', 'corporal', 'corporate', 'corporation', 'corps', 'corpses', 'corpus', 'correspondents', \"correspondents'\", 'corresponding', 'corresponds', 'corrupt', 'corse', 'corsica', 'cortex', 'cosmic', 'cosmological', 'cosmology', 'costa', 'costner', 'costs', 'costumes', 'cotton', 'couches', 'coughing', 'could', 'council', 'councillor', 'counted', 'countess', 'counties', 'countries', 'countries,', 'country', \"country's\", 'county', 'county,', 'coup', 'couple', 'course', 'courses', 'court', 'courteney', 'courtly', 'cousin', 'cousteau', 'couture', 'covenants', 'cover', 'coverage', 'covered', 'covers', 'cowboys', 'cowell', 'cox', 'coyote', 'cpc', 'crabs', 'craftsmen', 'craig', 'cranston', 'crap', 'crash', 'craven', 'crayfish', 'create', 'created', 'creates', 'creating', 'creation', 'creative', 'creator', 'creators', 'creature', 'credited', 'crediting', 'credits', 'creek', 'cressingham', 'crest', 'creston', 'cretton', 'crew', 'crime', 'crimes', 'criminal', 'crisis', 'criteria', 'criterion', 'critic', 'critical', 'critically', 'criticism', 'critics', \"critics'\", 'croatia', 'cronenberg', 'crops', 'crossover', 'crossroads', 'crouching', 'crowd', 'crowdsourced', 'crowe', 'crown', 'crows', 'crucial', 'crude', 'cruel', 'cruelty', 'cruise', 'crusoe', 'crustacean', 'crustaceans', 'cruz', \"cruz's\", 'cryer', 'crystal', 'cs', 'cs2', 'cthulhu', 'cuba', 'cuban', 'cuckoo', \"cuckoo's\", 'cuisine', 'culinary', 'cullen', 'cullinan', 'culminating', 'cult', 'cultists', 'cultivated', 'cultural', 'culture', 'cultured', 'cultures,', 'cup', 'cupbearer', 'cups', 'cups,', 'curacao', 'curaao', 'currency', 'current', 'currently', 'curtain', 'curtis', \"curtis'\", 'curve', 'custom', 'customary', 'cut', 'cuthbert', 'cw', 'cybernetic', 'cybill', 'cyclades', 'cyclically', 'cyclist', 'cypriot', 'cyprus', 'cyrus', 'czech', 'czechoslovakia', 'd', \"d'administration\", \"d'artagnan\", \"d'italia\", \"d'italiav\", \"d'onofrio\", \"d'or\", \"d's\", \"d'tat\", 'd.', 'd.c', 'd.c.', 'da', 'dacia', 'dactylic', 'dad', 'daddy', 'daenerys', 'dagworthy', 'dahl', 'daily', 'daimler', 'dakota', 'dale', 'dalgliesh', 'dalian', 'dalit', 'dallas', 'dallas,', 'damage', 'damala', 'dameron', 'damon', 'dan', 'dana', 'dance', 'dancer', 'dances', 'dancing', 'danes', 'dangerous', 'danica', 'daniel', 'danielle', 'daniels', 'danish', 'danny', \"dante's\", 'danvers', 'darcie', 'daredevil', 'dark', 'darker', 'darkness', 'darren', 'darrin', 'darshan', 'dashner', 'data', 'database', 'date', 'dates', 'dating', 'daughter', 'daughters', 'dave', 'david', 'davis', 'dawn', 'dawood', 'dawson', \"dawson's\", 'day', 'day,', 'days', 'daytime', 'dc', 'de', 'dead', 'dead:', 'deadliest', 'deadly', 'deakins', 'deal', 'deals', 'dean', 'death', 'deaths', 'deborah', 'debut', 'debuted', 'debuts', 'decade', 'decades', 'decapod', 'decapoda', 'decapods', 'deceased', 'deceive', 'december', 'deception', 'decided', 'decision', 'decisively', 'declaration', 'declare', 'declined', 'declines', 'declining', 'decreased', 'decreases', 'decter', 'dedicated', 'deemed', 'deep', 'deeper', 'deepest', 'deepika', 'deewani', 'defeat', 'defeated', 'defeating', 'defelice', 'defense', 'defiance', 'defined', 'defines', 'definitive', 'deformed', 'deformities', 'degeneres', 'degree', 'deity', 'deknight', 'del', 'deleted', 'delhi', 'deliberate', 'delivered', 'delivery', 'dell', \"dell'arte\", 'deluxe', 'dementia', 'demi', 'demigod', 'democracy', 'democrat', 'democratic', 'democrats', 'demographic', 'demographics', 'demolished', 'demonologist', 'demonologists', 'demons', 'demonstrate', 'demonstrated', 'denied', 'denise', 'denizens', 'denmark', 'denmark,', 'dennehy', 'dennis', 'denominated', 'densely', 'dent', 'depardieu', 'department', 'departments', 'dependent', 'depicted', 'depicts', 'depression', 'deprive', 'deputy', 'der', 'derided', 'derivative', 'derivatives', 'derived', 'derives', 'des', 'desaulniers', 'descendants', 'descent', 'describe', 'described', 'descriptions', 'desert', 'design', 'designated', 'designation', 'designed', 'designer', 'designers', 'designs', 'desire', 'desk', 'desktop', 'despised', 'despite', 'dessalines', 'destin', 'destination', 'destinations', 'destroy', 'destroyed', 'destructive', 'details', 'detective', 'detention', 'determine', 'determined', 'detroit', 'deutch', 'dev', 'develop', 'developed', 'developer', 'developing', 'development', 'development,', 'developmental', 'device', 'devices', 'devil', 'devils', 'devoid', 'devoted', 'devotion', 'devotions', 'dewitt', 'dexter', 'dharmesh', 'diagnosed', 'diagnosis', 'dial', 'diamond', 'diana', \"diana's\", 'diaries', 'diarrhea', 'diaz', 'diazepine', 'dicaprio', 'dichotomous', \"dicken's\", 'dickens', 'dictated', 'dictator', 'dictatorship', 'dictyostelium', 'did', 'did,', 'didi', 'die', 'died', 'diet', 'differ', 'differences', 'different', 'difficult', 'difficulties', 'difficulty', 'digger', 'digital', 'dili', 'dilwale', 'dimaggio', 'ding', 'dini', 'dining', 'dinner', 'dione', 'diplomat', 'diplomatic', 'direct', 'directed', 'directer', 'directing', 'direction', 'directly', 'director', 'director,', 'directorial', 'directors', 'directs', 'dirty', 'disappeared', 'disapproval', 'disassociated', 'disaster', 'disbanded', 'disc', 'discharge', 'discipline', 'disciplines', 'disco', 'discomfort', 'disconnect', 'discontinued', 'discover', 'discovered', 'discovering', 'discovery', 'discovery,', 'discrimination', 'discuss', 'disease', 'disestablished', 'dish', 'dishevelled', 'dishonorable', 'disinformation', 'disk', 'disney', 'disneyland', 'disorder', 'disorder,', 'disorders', 'displayed', 'displays', 'disputed', 'disruptions', 'dissociated', 'dissociation', 'dissociative', 'dissolution', 'dissolved', 'distance', 'distilled', 'distinct', 'distinctions', 'distinguishable', 'distinguished', 'distinguishing', 'distress', 'distressing', 'distributed', 'distributes', 'distribution', 'distributor', 'district', 'ditching', 'divergent', 'diverse', 'diversified', 'divested', 'divided', 'divides', 'divine', 'division', 'divisions', 'divorce', 'divorced', 'dj', 'do', 'dobrudja', 'doctor', \"doctor's\", 'doctorow', 'doctrine', 'docudrama', 'documentary', 'documents', 'dodgeball', 'dodgeball:', 'dodo', 'does', \"doesn't\", 'dog', 'dogs', 'dogtown', 'doha', 'doll', 'dollar', 'dollars', 'dollhouse', 'dolls', 'domestic', 'dominant', 'dominated', 'dominion', 'dominions', 'don', 'donald', 'donated', 'donates', 'donations', 'done', 'dongri', 'donna', 'donoghue', 'door', 'dopamine', 'doping', 'dorothea', 'dorrit', 'dorsey', 'doses', 'dota', 'double', 'doubles', 'doug', 'down', 'downfall', 'download', 'downtown', 'doxycycline', 'dozen', 'dq', 'dr.', 'draft', 'dragon', 'drake', \"drake's\", 'drakken', 'drama', 'drama,', 'dramas', 'dramas,', 'dramatic', 'dramatically', 'dramatist', 'dramatizes', 'dramedy', 'dravidian', 'draw', 'dreamliner', 'dreams', 'dreamworks', 'drescher', 'dress', 'drew', 'dreyfus', 'drift', 'drink', 'drive', 'driven', 'driver', 'driving', 'dropped', 'drug', 'drugs', 'drummer', 'drummond', 'drunk', 'du', 'dual', 'duality', 'duane', 'dubbed', 'dubna', 'dubs', 'duchess', 'duchovny', 'duchy', 'duck', 'dudley', 'due', 'duke', 'dulhania', 'dulquer', 'dumont', 'dumraon', 'dunaway', 'dunbar', 'dunder', 'dunst', 'duo', 'duration', 'durham', 'during', 'durnsford', 'dustin', 'dutch', 'duties', 'duty', 'duvall', 'dvd', 'dwarf', 'dwellings', 'dying', 'dylan', 'dynamic', 'dynamics', 'dynasty', 'dysentery', 'dyspnea', 'dystopian', 'dn', 'e', \"e's\", 'e.', 'e.g.', 'e4', 'each', 'eagle', 'ear', 'earhart', 'earl', 'earle', 'earlier', 'earliest', 'early', 'earned', 'earnest', 'earning', 'earnings', 'earnshaw', 'earth', \"earth's\", 'earthling', 'earthquake', 'earthquakes', 'east', 'eastbay/footlocker', 'easterly', 'eastern', 'eastwood', 'eat', 'ebon', 'eccentric', 'economic', 'economically', 'economics', 'economies', 'economist', 'economy', 'ed', 'eddard', 'edge', 'edinburgh', 'edison', 'edited', 'editing', 'edition', 'editor', 'edits', 'educated', 'education', 'educational', 'edward', 'eels', 'effect', 'effective', 'effects', 'effie', 'effort', 'efforts', 'effy', 'efron', \"efron's\", 'egg', 'ego', 'egypt', \"egypt's\", 'egyptian', 'egyptians', 'eichinger', 'eight', 'eighth', 'eighties', 'either', 'ekta', 'elaborate', 'elaine', 'elder', 'elderly', 'eldest', 'elect', 'elected', 'election', 'elections', 'electric', 'electrical', 'electrocuted', 'electrocution', 'electrodes', 'electromagnetism', 'electronic', 'electronica', 'electronics', 'elegantly', 'element', 'element,', 'elemental', 'elements', 'elephant', 'elevators', 'eleven', 'eleventh', 'elias', 'eligible', 'elimination', 'eliot', 'elisabeth', 'elisha', 'elite', 'elizabeth', 'ella', 'ellen', 'elongated', 'eloped', 'else', 'elucidation', 'elwes', 'email', 'emas', 'embayment', 'embodying', 'embolism', 'embroiled', 'emerged', 'emerging', 'emerick', 'emery', 'emeryville', 'emigrated', 'emilia', 'emilio', 'emily', 'emily,', 'eminem', 'emirates', 'emm', 'emma', 'emmanuel', 'emmitt', 'emmons', 'emmy', 'emotional', 'emotions', 'empath', 'emperor', 'emperors', 'emperors,', 'emphasis', 'empire', 'empires', 'employed', 'employee', 'employees', 'employing', 'empowerment', 'empresa', 'en', 'enables', 'enclave', 'encompassed', 'encompasses', 'encompassing', 'encountering', 'encyclopedia', 'end', 'ended', 'ending', 'endings', 'endocrine', 'ends', 'enduring', 'enemies', 'enemy', 'energetic', 'energy', 'enforcement', 'engaged', 'engages', 'engine', 'engined', 'engineer', 'engines', 'england', 'english', 'enhanced', 'enhancement', 'enhancer', 'enhances', 'enjoying', 'enlisted', 'ennis', 'enormous', 'enosis', 'enough', 'ensemble', 'enter', 'entered', 'entering', 'entertainer', 'entertainers', 'entertainment', 'entertainment,', 'entire', 'entirely', 'entirety', 'entities', 'entity', 'entrepreneur', 'entries', 'entry', 'environment', 'ep', 'ephron', 'epic', 'episcopal', 'episode', 'episodes', 'epistolary', 'epitomizing', 'eponymous', 'eps', 'equality', 'equated', 'equations', 'equipment', 'equivalent', 'era', 'eradicated', 'eric', 'erin', 'eris', 'eritrea', 'erlangen', 'erotic', 'erstwhile', 'erupted', 'eruptions', 'esarhaddon', 'escalators', 'escaping', 'escobar', 'especially', 'espenson', 'espn', 'espritus', \"esquire's\", 'esquires', 'essandoh', 'essay', 'essayist', 'essence', 'essex', 'establish', 'established', 'establishing', 'establishment', 'esteban', 'estelle', 'estevez', 'estimate', 'estimated', 'estonia', 'estonian', 'estrada', 'estranged', 'etc.', 'ethan', 'ethereal', 'ethics', 'ethiopia', 'ethiopian', 'ethnic', 'ethnically', 'etienne', 'etymology', 'eugene', 'eugne', 'euphemia', 'eurasia', 'europe', 'europe,', 'european', 'eurotas', 'eurozone', 'eurythmics', 'eustatius', 'evaluating', 'evan', 'evangelical', 'evans', 'evanston', 'even', 'event', 'eventbrite', 'events', 'eventually', 'ever', 'everdeen', 'every', 'everybody', 'everyday', 'everyone', 'everywhere', 'evil', 'evoked', 'evolution', 'evrotas', 'ex', 'exact', 'exactly', 'exaggerated', 'examination', 'example', 'excellence', 'except', 'exception', 'exchange', 'exclude', 'excluded', 'excludes', 'exclusive', 'exclusively', 'execute', 'executed', 'executes', 'executes,', 'execution', 'executive', 'exelon', 'exercise', 'exercised', 'exercises', 'exerted', 'exhibited', 'exhibition', 'exhumation', 'exhume', 'exhumed', 'exist', 'existed', 'existence', 'existing', 'exists', 'exit', 'exited', 'exonymic', 'exotic', 'expand', 'expanded', 'expanding', 'expected', 'expelled', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experimental', 'experimentation', 'experiments', 'experiments,', 'expert', 'expired', 'explicit', 'explored', 'explorer', 'explores', 'explosive', 'exponents', 'export', 'exports', 'exposition', 'express', 'expressed', 'expression', 'extended', 'extending', 'extensive', 'extent', 'extinct', 'extinction', 'extinguished', 'extra', 'extreme', 'extremely', 'extremists', 'exupry', 'eyes', 'f.', 'f.c', 'f.c.', 'face', 'facetime', 'facilities', 'facility', 'facinelli', 'factions', 'facto', 'factor', 'factories', 'factors', 'faculty', 'faded', 'fagerbakke', 'failed', 'failure', 'failures', 'fairchild', 'faith', 'faithful', 'faking', 'falcon', 'falcn', 'fall', 'falls', 'false', 'fame', 'famed', 'familiar', 'families', 'family', 'famous', 'fan', 'fanaa', 'fanatic', 'fantasy', 'far', 'fare', 'fargo', \"fargo's\", 'faris', 'farmer', 'farmhouse', 'farms', 'faroe', 'fascist', 'fashion', 'fashioned', 'fassbender', 'fast', 'fastened', 'fastest', 'fasting', 'fat', 'fatal', 'father', 'fathers', 'fatigue', 'faust', 'favor', 'favorable', 'favorably,', 'favored', 'favoring', 'favorite', 'favors', 'favoured', 'favours', 'faye', 'fbi', 'fc', 'feared', 'feast', 'feathers', 'feature', 'featured', 'features', 'featuring', 'february', 'fechter', 'federal', 'federation', 'federation/world', 'fee', 'feeding', 'feel', 'feet', 'feldstein', 'fell', 'fellow', 'felsic', 'female', 'females', 'fence', 'fenix', 'fenn', 'fennel', 'fenway', 'ferdinand', 'fergana', 'fernando', 'ferrari', 'festival', 'festive', 'feud', 'fever', 'few', 'fewer', 'fiction', 'fiction,', 'fictional', 'fictitious', 'fidaa', 'field', 'fields', 'fifa', 'fifteen', 'fifteenth', 'fifth', 'fifties', 'fifty', 'fight', 'fighters', 'fighting', 'fights', 'figure', 'figures', 'fiji', 'file', 'files', 'filings', 'film', 'film,', 'filmed', 'filmfare', 'filming', 'filmmaker', 'filmmakers', 'filmmaking', 'films', 'final', 'finale', 'finally', 'finals', 'finance', 'financed', 'finances', 'financial', 'financially', 'financier', 'financing', 'financing,', 'find', 'finds', 'fine', 'fingerprinting', 'finis', 'finished', 'fink', 'finland', 'finnish', 'fiona', 'fire', 'firefly', 'firefox', 'firms', 'first', \"first's\", 'fish', 'fisher', 'fission', 'fitted', 'five', 'fixed', 'flagship', 'flahive', 'flamingo', 'flat', 'flavor', 'flavoured', 'fled', 'fleet', 'fleming', 'flew', 'flickerman', 'flight', 'flights', 'flinstones', 'flintstones', 'floated', 'flog', 'floors', 'flop', 'flopped', 'flora', 'florida', 'florida,', 'flourished', 'flowering', 'flowers', 'floyd', 'fluent', 'fluid', 'fly', 'flynn', \"flynn's\", 'focal', 'focus', 'focused', 'focuses', 'focusing', 'fog', 'folds', 'folk', 'folklore', 'follow', 'followed', 'followers', 'following', 'follows', 'foo', 'food', 'foods', 'foot', 'footaction', 'footage', 'football', 'footballer', 'footloose', 'footprint', 'footsteps', 'footwear', 'for', 'forbes', 'forbids', 'force', 'forces', 'ford', 'forecasts', 'foreign', 'foreigner', 'foreplay', 'forest', 'forested', 'forever', 'forger', 'forgetting', 'forgives', 'form', 'formal', 'formally', 'forman', 'format', 'formation', 'formats', 'formed', 'former', 'formerly', 'forms', 'formula', 'forrest', 'fort', 'forth', 'forthcoming', 'fortress', 'fortus', 'forty', 'forum', 'forward', 'foster', 'fought', 'foul', 'found', 'foundation', 'founded', 'founder', 'founding', 'four', 'fourteen', 'fourth', 'fox', 'foxx', 'fragments', 'frame', 'framed', 'frames', 'framing', 'fran', 'franc', 'franca', 'france', 'frances', 'franchise', 'franchised', 'francis', 'francisco', 'franco', 'frank', 'frankenstein', 'franklin', 'franz', 'fraternity', 'fraud', 'freak', 'frears', 'fred', 'freddie', 'frederator', 'frederick', 'free', 'freedom', 'freely', 'freemason', 'freer', 'freighter', 'french', 'frequent', 'frequently', 'fresh', 'friary', 'friday', \"friday's\", 'friedrich', 'friend', 'friendly', 'friends', 'friendship', 'fright', 'fringe', 'fritton', 'fritz', 'frog', 'frogs', 'from', 'front', 'fronted', 'frontrunner', 'frost', 'frosty', 'frs', 'frse', 'frsl', 'fruit', 'fruits', 'frusciante', 'frdric', 'ft', 'fu', 'fulfilling', 'full', 'fuller', 'fully', 'function', 'functional', 'functionality', 'functioning', 'functions', 'fundamental', 'fundamentals', 'funded', 'funeral', 'fungi', 'funk', 'fur', 'further', 'furthermore', 'fury', 'fusion', 'futurama', 'future', 'fx', 'fx/fxx', 'fh', 'fted', 'g', 'g.', 'g.p.', 'gabon', 'gabriel', 'gaffigan', 'gagliardi', 'gaiman', 'gain', 'gained', 'gaining', 'gaius', 'galactic', 'galaxy', 'gale', 'galen', 'galicia', 'gallagher', 'galt', 'gama', 'gamal', 'game', 'games', \"games'\", 'gaming', 'gang', 'ganges', 'gangster', 'ganymede', 'gaona', 'garcia', 'garden', 'garner', 'garnered', 'garnering', 'garrison', 'gary', 'gassmann', 'gaston', 'gathered', 'gathering', 'gaul', 'gaulle', 'gave', 'gavin', 'gay', 'gdp', 'gears', 'gecko', 'gein', \"gein's\", 'gem', 'gemini', 'gems', 'gemstone', 'gena', 'gender', 'gene', 'general', 'generally', 'generals', 'generate', 'generated', 'generic', 'genetic', 'genndy', 'genocide', 'genocides', 'genre', 'genres', 'gensakusha', 'genus', 'geodetic', 'geoghan', 'geographic', 'geography', 'geopolitical', 'geopolitics', 'george', 'georges', 'georgia', 'georgian', 'georgians', 'geraldine', 'gerard', 'gerhard', 'german', 'germanic', 'germanicus', 'germany', 'germany,', 'gerome', 'get', 'gettleman', 'getty', \"getty's\", 'gettysburg', 'gham', 'ghar', 'ghazi', 'ghost', 'ghost,', 'ghosts', 'giant', 'gibbons', 'gibraltar', 'gigantic', 'gilbert', 'gillan', 'gilles', 'gillian', 'gilmore', \"gilmore's\", 'gilmour', 'girl', 'girlfriend', \"girlfriend's\", 'girls', \"girls'\", 'giro', 'giselli', 'give', 'given', 'giver', 'gives', 'glacial', 'glaciation', 'glacier', 'glamour', 'glands', 'glasgow', 'glass', 'gleason', 'glenn', 'glided', 'global', 'globalism', 'globalization', 'globally', 'globe', 'globes', 'glow', 'gluck', 'glutamate', 'gluten', 'gm', 'gnaw', 'gneiss', 'gnosticism', 'gnr', 'go', 'goal', 'goals', 'god', 'godchaux', 'goddard', 'goddess', 'goddesses', \"goddesses'\", 'gods', 'goebbels', \"goebbels'\", 'goebbels,', 'goes', 'goethe', 'going', 'gold', 'goldberg', 'golden', 'goldie', 'goldmines', 'goldwyn', 'golf', 'golf,', 'goliyon', 'gonads', 'gone', 'gonorrhea', 'good', 'goodman', 'goodson', 'google', 'gorai', 'gorilla', 'gory', 'goscinny', 'gosling', 'gospel', 'gosudarstvennoy', 'got', 'gotham', 'gothic', 'goths', 'gotten', 'gough', 'governance', 'governed', 'governess', 'governing', 'government', 'government,', 'governments', 'governor', 'gps', 'gracie', 'gradually', 'graduate', 'graduated', 'graduating', 'graduation', 'graham', 'grain', 'grammy', 'grand', 'grandchild', 'grandfather', 'grandin', 'grandma', 'grandmother', 'grandparents', 'grandson', 'granite', 'granite,', 'granitic', 'grant', 'granted', 'granular', 'graphic', 'graphics', 'grappling', 'grassy', 'grateful', 'grave', 'graveyards', 'gray', 'grays', 'grazer', \"grazer's\", 'grease', \"grease's\", 'great', 'greater', 'greatest', 'greatly', 'greco', 'greece', \"greece's\", 'greek', 'green', 'greengrass', 'greenland', 'greenwalt', 'greenwood', 'greg', 'gregory', 'grenada', 'grew', 'grey', \"grey's\", 'greyfriars', 'greyhound', 'grief', 'griffin', 'griffiths', 'grime', 'gritty', 'groban', 'grohl', 'groove', 'grooves', 'gross', 'grossed', 'grossing', 'ground', 'group', 'groups', 'growing', 'grown', 'growth', 'gruff', 'guadalete', 'guard', 'guardians', 'guards', 'guatemalan', 'guerrero', \"guerrero's\", 'guess', 'guest', 'guggenheim', 'guidelines', 'guiding', 'guild', 'guillem', 'guilty', 'guinness', 'guitar', 'guitarist', 'gulf', 'gump', 'gunn', 'guns', 'gurkani', 'gus', 'guthrie', 'guy', 'guys', 'guzmn', 'gyllenhaal', 'h', 'h.', 'ha', 'habitable', 'habits', 'habsburg', 'had', 'hader', 'hades', 'hadley', 'hagan', 'hagen', 'haggerty', 'hai', 'hain', 'hair', 'hairy', 'haiti', 'haitian', 'hal', 'hale', \"hale's\", 'half', 'halfway', 'hall', 'halloween', 'halted', 'hamilton', 'hamlet', 'hamm', 'hammer', 'hammond', 'hampton', 'hamptons', 'hand', 'handed', 'handheld', 'handling', 'hands', 'hanged', 'hangover', 'hannah', 'hannibal', 'hannibalic', 'hanover', 'hans', 'hansol', 'happened', 'happy', 'hard', \"hard's\", 'harder', 'hardware', 'hardwicke', 'hardy', 'harland', 'harlem', 'harmony', 'harper', 'harrelson', 'harris', 'harrison', \"harrison's\", 'harrisongot', 'harry', 'harvest', 'harvest,', 'harvey', 'haryana', 'has', 'has,', 'haseeno', 'haskin', \"hasn't\", 'hasselhoff', 'hate', 'hatfield', 'hatfields', 'hathaway', 'hati', \"hati's\", 'hatred', 'haul', 'haumea', 'haunted', 'hauntings', 'haute', 'have', 'having', 'hawaii', 'hawkins', 'hawley', 'hawn', 'hawthorne', 'hayek', 'hayes', \"hayes'\", 'haymitch', 'hays', 'hbo', 'he', 'head', 'headaches', 'headlined', 'headquartered', 'headquarters', 'heads', 'headstrong', 'health', 'health,', 'healthy', 'heard', 'hears', 'heart', 'hearts', 'heat', 'heath', 'heather', 'heatherette', 'heaven', 'heavensbee', 'heavily', 'heavy', 'heavyweight', 'hebe', \"hebe's\", 'hebrew', 'heckerling', 'hedges', 'hedi', 'hedonistic', 'height', 'heighten', 'heightened', 'heights', 'heigl', 'heiress', 'heirs', 'heist', 'held', 'helen', 'helena', 'helio', 'hell', 'help', 'helped', 'helping', 'helps', 'hemisphere', 'hemsworth', 'hendrix', 'henri', 'henricus', 'henry', 'henson', 'hephaestus', 'her', 'hera', 'heracles', 'herbert', 'herbivore', 'herbs', 'here', 'herek', 'heritage', 'hermann', 'hermit', 'hernndez', 'hero', 'heroes', \"heroes'\", 'heroine', 'herrington', 'herrmann', 'herself', 'hertford', 'hertfordshire', 'herzogenaurach', 'heston', 'hexameter', 'hexapods', 'hezbollah', 'hiatus', 'hibbert', 'hickox', 'hidden', 'hide', 'hierarchy', 'higgins', 'high', 'higher', 'highest', 'highlander', 'highly', 'highmore', 'highway', 'highways', 'hikaru', 'hill', 'hillary', 'hillbillies', 'hillenburg', 'him', 'himself', 'hindenburg', 'hindi', 'hindrance', 'hindu', 'hinduism', \"hinduism's\", 'hindustani', \"hindustani's\", 'hinged', 'hinton', 'hip', 'hire', 'hired', 'his', 'hispania', 'hispanic', 'hispano', 'histamine', 'historian', 'historians', 'historic', 'historical', 'historically', 'history', 'hit', 'hitchcock', 'hits', 'hizballah', 'hizbullah', 'hockey', 'hodges', 'hoff', 'hoffman', 'hogan', 'hokkaido', 'hold', 'holding', 'holds', 'holes', 'holiday', 'holidays', 'holiest', 'holland', 'hollow', 'holly', 'hollywood', 'holmes', 'holographic', 'holy', 'home', 'homer', \"homer's\", 'homes', 'hometown', 'hominoidea', 'homogeneous', 'homophonic', 'homosexual', 'homosexuality', 'honeymoon', 'hong', 'honor', 'honorable', 'honored', 'honors', 'honour', 'honshu', 'hood', 'hoops', 'hoover', 'hop', 'hop,', 'horne', 'horrible', \"horrible's\", 'horribly', 'horror', 'horse', 'hospital', 'hospitality', 'hospitaller', 'host', 'hosted', 'hosts', 'hot', 'hota', 'hotel', 'hotels', 'hotta', 'hour', 'hours', 'house', 'housefull', 'household', 'households', 'houses', 'housesitter', 'houston', 'houston,', 'how', 'howard', 'however', 'howl', 'hoyts', 'hub', 'hubbard', 'hudson', 'hugh', 'hughes', 'hulk', 'hum', 'human', 'humane', 'humanitarian', 'humanity', 'humans', 'humayun', 'hummer', 'humorous', 'humphrey', 'hundred', 'hundreds', 'hungary', 'hunger', 'hunter', 'hunters', 'hunting', 'hurricane', 'hurt', 'hurwitz', 'husband', 'hussein', 'husson', 'hutchence', 'hutcherson', 'hutu', 'huxley', 'hybrid', 'hydro', 'hyksos', \"hyksos'\", 'hyland', 'hyper', 'hypersaline', 'hypertext', 'hypothetical', 'hchstadt', 'i', \"i'm\", \"i's\", 'i,', 'i.', 'i.e.', 'ian', 'iberia', 'iberian', 'ibn', 'ibrahim', 'ibs', 'ice', 'iceland', \"iceland's\", 'icelandic', 'iconic', 'idaho', 'ideas', 'identification', 'identified', 'identifies', 'identify', 'identities', 'identity', 'ideologies', 'idiom', 'idol', 'if', 'igneous', 'ignored', 'ignoring', 'igor', 'ii', 'iii', \"iii's\", 'iliad', 'illinois', 'illiterate', 'illness', 'illnesses', 'illustrated', 'illustrations', 'illustrator', 'image', 'images', 'imaginarium', 'imagination', 'imax', 'imdb', 'imdb.com', 'immediately', 'immigration', 'immortal', 'immortalized', 'immune', 'impact', 'impeded', 'imperative', 'imperial', 'impersonated', 'implants', 'implements', 'importance', 'important', 'imported', 'imports', 'impossible', 'improve', 'improves', 'in', 'inaugural', 'inaugurated', 'inc', 'inc.', 'incapable', 'incarnation', 'inception', 'include', 'included', 'includes', 'including', 'inclusion', 'inclusively', 'income', 'incorporate', 'incorporated', 'incorporates', 'incorporation', 'incorrectly', 'increase', 'increased', 'increases', 'increasing', 'incredibles', 'incredibly', 'incurs', 'indefatigable', 'indefinitely', 'independence', 'independent', 'independently', 'indeterminates', 'index', 'india', 'indian', 'indiana', 'indianapolis', 'indians', 'indicated', 'indies', 'indistinguishable', 'inditex', 'individual', 'individuals', 'indivisible', 'indo', 'indonesia', 'indonesian', 'induce', 'induced', 'induces', 'inducted', 'inductees', 'industrial', 'industrialised', 'industrialization', 'industrialized', 'industries', 'industry', 'indy', 'infantry', 'infections', 'inferior', 'inferno', 'inferred', 'influence', 'influenced', 'influences', 'influential', 'informally', 'information', 'ingredients', 'inhabitants', 'inhabited', 'initial', 'initialism', 'initially', 'initials', 'injection', 'injuries', 'inland', 'inmates', 'inner', 'innes', 'innocence', 'innocent', 'innovation', 'innovative', 'innovator', 'inscribed', 'insertion', 'inside', 'inspector', 'inspectorate', 'inspiration', 'inspired', 'installment', 'installments', 'instance', 'instead', 'institute', 'institution', 'institutions', 'instructional', 'instructor', 'instructors', 'instrumental', 'instrumentalists', 'instruments', 'insular', 'insurance', 'insurgent', 'integer', 'integrated', 'intellectual', 'intelligence', 'intended', 'intensifies', 'intensity', 'intentionally', 'interaction', 'interactions', 'interactive', 'intercollegiate', 'interconnected', 'intercontinental', 'interdependent', 'interest', 'interests', 'intergovernmental', 'interior', 'interlinked', 'interludes', 'intermedio', 'intern', 'internal', 'international', 'internationally', 'interned', 'internet', 'interning', 'interrelate', 'interrupted', 'interscope', 'intertwined', 'interventionism', 'intestinal', 'intimate', 'into', 'intrigues', 'introduced', 'introduction', 'intrusive', 'invading', 'invasion', 'invent', 'invented', 'invention', 'inventor', 'invents', 'investigate', 'investigated', 'investigates', 'investigation', 'investigations', 'investigators', 'investment', 'invisible', 'involuntary', 'involve', 'involved', 'involvement', 'involves', 'involving', 'ionesco', 'ionian', 'ios', 'iphone', 'iran', 'iranian', 'iranic', 'iraq', 'iraqi', 'ireland', 'irish', 'iron', 'irons', 'is', 'isaac', 'isabel', 'isabella', 'isabelle', 'ishq', 'islam', 'islamabad', 'islamic', 'islamist', 'island', 'island,', 'islands', 'islands,', 'isle', 'ismael', \"isn't\", 'isolated', 'isolation', 'isotopes', 'issa', 'issue', 'issued', 'issues', 'it', \"it's\", 'italian', 'italy', 'item', 'items', 'its', 'itself', 'ittefaq', 'iv', 'iv,', 'ivan', 'ives', 'ivor', 'ivy', 'ix', 'j.', 'j.d.', 'j.j.', 'j.k.', 'j.r.r.', 'jaan', 'jab', 'jabbar', 'jack', 'jackets', 'jackie', 'jackson', 'jacob', 'jacobs', 'jacobson', 'jacobus', 'jacques', 'jada', 'jade', 'jagger', 'jaime', 'jainulabdeen', 'jake', 'jamaica', 'jamal', 'james', 'james,', 'jamestown', 'jamie', 'jamiroquai', 'jan', 'janata', 'jane', 'janet', 'janitor', 'janney', 'january', 'japan', \"japan's\", 'japanese', 'jaqueline', 'jarhead', 'jarhead,', 'jarmusch', 'jason', 'java', 'jawaani', 'jay', 'jayden', 'jayenge', 'jazz', 'jean', 'jeanette', 'jeb', 'jed', 'jeff', 'jefferson', 'jeffrey', 'jenna', 'jennifer', 'jenny', 'jeopardy', 'jeremy', 'jerome', 'jerry', 'jersey', 'jerusalem', 'jesse', 'jessica', 'jesus', 'jet', 'jewelry', 'jewish', 'jews', 'jfk', 'jiang', 'jim', 'jimi', 'jingle', 'jitsu', 'jiu', 'jj', 'joanna', 'job', 'jobs', 'jodha', 'joe', 'joel', 'joffrey', 'johann', 'johannes', 'johansson', 'johar', 'john', 'johnny', 'johnson', 'join', 'joined', 'joining', 'joins', 'joint', 'jointly', 'jon', 'jonah', 'jonas', 'jonathan', 'jones', \"jones's\", 'joni', 'jordan', 'jose', 'joseon', 'joseph', 'josh', 'joshua', 'joss', 'journalist', 'journalists', 'journals', 'journey', \"journey's\", 'joy', 'jr', 'jr.', 'judaism', 'judd', 'judge', 'judged', 'judges', 'judgments', 'judiciary', 'judith', 'judo', 'jugoslavija', 'julia', 'julian', 'julie', 'juliet', 'juliette', 'julio', 'julius', 'july', 'jump', 'june', 'junior', 'juniors', 'junta', 'jupiter', 'juridical', 'jurisdiction', 'jurist', 'just', 'justice', 'justin', 'k.', 'ka', 'kabhi', 'kabhie', 'kadavul', 'kahaani', 'kahlifa', 'kajol', 'kal', 'kalam', 'kallis', 'kambakkht', 'kameni', 'kammula', 'kanon', 'kapoor', 'kara', 'karan', 'kareem', 'kareena', 'karen', 'karl', 'kartli', 'kartvelians', 'kasam', 'kasautii', 'kashmiri', 'kaskar', 'kassell', 'kate', 'katherine', 'kathleen', 'katie', 'katniss', 'kavach', 'kay', 'kaya', 'kayle', 'kazakhstan', 'kc', 'keaton', 'keegan', 'keener', 'keeps', 'keepsakes', 'keith', 'kelland', 'kelly', 'ken', 'kennedy', 'kenneth', 'kenney', 'kenny', 'kent', 'kentucky', 'kentucky,', 'kenya', 'kept', 'kerala', 'kerkovich', 'kerri', 'kesey', 'kevin', 'key', 'keyboardist', 'keyboards', 'kgb', 'khalifa', 'khama', 'khan', 'khanate', 'khanna', 'khomeini', 'khushi', 'ki', 'kids', 'kiefer', 'kieslowski', 'kievan', 'kilik', 'kill', 'killed', 'killer', 'killing', 'kills', 'kim', 'kimmy', 'kind', 'king', 'kingdom', 'kingdoms', 'kings', 'kingsman', 'kinnear', 'kinsella', 'kirk', 'kirsten', 'kitchen', 'kitsis', 'klaus', 'km', 'km2', 'knew', 'knight', 'knights', 'knocked', 'knockouts', 'know', 'knowledge', 'known', 'ko', 'koala', 'kodokan', 'kojol', 'komitet', 'konami', 'kong', 'konidela', 'kontinen', \"kontinen's\", 'korea', 'korean', 'korn', 'kos', 'koteas', 'krabs', 'kraglin', 'kramer', 'kretschmann', 'krill', 'kring', 'kripke', 'kristen', 'kroll', 'krumholtz', 'krzysztof', 'kubiszewski', 'kubrick', 'kuch', 'kuei', 'kuklinski', 'kulkarni', 'kumar', 'kumkum', 'kung', 'kurgan', 'kurt', 'kurtzman', 'kwesi', 'kyle', 'kylie', 'kyoto', 'kyra', 'kyrgyzstan', 'kyunki', 'kyushu', \"l'esprit\", 'l.', 'l1', 'la', 'label', 'labeled', 'labels', 'laboratories', 'laboratory', 'labour', 'lack', 'lacked', 'lacking', 'lacks', 'laconia', \"laconia's\", 'lady', 'lafangey', 'lafayette', 'lafever', 'lagerlf', 'lagravenese', 'lagte', 'lahore', 'laid', 'lake', 'lakeith', 'lakers', 'lambert', \"lambert's\", 'lambs', 'lamp', 'lana', 'lance', 'land', 'landed', 'landform', 'landhelgisgaeslan', 'landing', 'landlocked', 'landmark', 'landmass', 'landscape', 'landscapes', 'lane', \"lane's\", 'lanes', 'lang', 'language', 'languages', 'lani', 'lanka', 'lannister', 'lapaglia', 'larenz', 'large', 'largely', 'larger', 'largest', 'lars', 'larson', 'las', 'last', 'lasted', 'lasting', 'lasts', 'late', 'later', 'lateran', 'latest', 'latin', 'latter', 'latvia', 'latvian', 'lauded', 'laugh', 'launched', 'laura', 'lauren', 'laurent', 'laurie', 'lava', 'lavigne', 'law', 'lawful', 'lawrence', 'laws', 'lawson', 'lawsuit', 'lawyer', 'lawyers', 'lawyers.', 'laying', 'layout', 'laz', 'le', 'leachman', 'lead', 'leader', 'leaders', 'leadership', 'leading', 'leads', 'league', 'leagues', 'learned', 'learning', 'leasing', 'leasing,', 'least', 'leave', 'leaves', 'leaving', 'lebanon', 'lebowski', 'lech', 'lecturer', 'led', 'ledger', 'lee', \"lee's\", 'lee,', 'leela', 'leeward', 'left', 'leg', 'legacy', 'legal', 'legalist', 'legally', 'legend', 'legendary', 'legions', 'legislation', 'legislative', 'legislature', 'legitimate', 'lego', 'legs', 'leicester', 'leigh', 'leigh,', 'leiter', 'lem', 'lemore', 'length', 'lengthened', 'lengthening', 'lengthwise', 'lennix', 'lennox', 'lenox', 'lens', 'leo', 'leonard', 'leonardo', 'leopards', 'leslie', 'less', 'lessen', 'lesser', 'let', 'lethal', 'leto', 'letter', 'letters', 'levamfetamine', 'level', 'levels', 'levinson', 'levoamphetamine', 'lewis', 'lex', 'liam', 'liberal', 'libertarian', 'library', 'libre', 'licence', 'license', 'license,', 'licensed', 'licensing', 'lie', 'liechtenstein', 'lien', 'lies', 'life', 'lifelong', 'light', 'lighter', 'lightfoot', 'lightning', 'like', 'lillard', 'lily', 'limitations', 'limited', 'limits', 'lincoln', 'lincolnshire', 'lincolnshire,', 'lindbergh', 'lindelof', 'line', 'lineal', 'liner', 'linked', 'linkedin', 'linkin', 'linklater', 'links', 'linux', 'lionel', 'lions', 'lionsgate', 'liquid', 'list', 'listed', 'listin', 'listings', 'lists', 'literary', 'literature', 'literature,', 'litigation', 'little', 'littlefinger', 'live', 'lived', 'liver', 'lives', 'livestock', 'living', 'livingston', 'liz', 'lizzie', 'lizzy', 'llama', 'llc', 'llewyn', 'lloyd', 'lobsters', 'local', 'locality', 'localized', 'located', 'location', 'locations', 'locker', \"locker's\", 'lockheed', \"lockheed's\", 'lodomeria', 'lofland', 'loftiest', 'logie', 'lois', 'lola', 'london', 'london,', 'loneliness', 'lonely', 'lonergan', 'long', 'longer', 'longest', 'longitude', 'longtime', 'look', 'loose', 'loosely', 'lord', 'lorde', 'lords', 'lordship', 'lorelai', 'lorenz', 'lorenzo', 'loretta', 'lorraine', 'los', 'lose', 'losing', 'loss', 'losses', 'lost', 'lot', 'lots', 'loud', 'louie', 'louis', 'louise', 'louisiana', 'love', \"love's\", 'lovecraft', 'lovecraftian', 'loved', 'lover', 'low', 'lowden', 'lowe', 'lowell', 'lower', 'lowering', 'lowest', 'lp', \"lp's\", 'lps', 'lrh', 'lt.', 'lucas', 'lucha', 'lucille', 'lucky', 'lucy', 'ludacris', 'ludwig', 'luen', 'luetkemeyer', 'luis', 'luiz', 'lumley', 'lund', 'lundin', 'lung', 'lush', 'luther', 'luthor', 'luxury', 'lying', 'lymphoma', 'lyn', 'lynch', 'lynn', 'lynyrd', 'lyon', \"lyon's\", 'lyonne', 'lyr', 'lyric', 'lyricist', 'lyrics', 'm', 'm.', 'ma', 'mac', 'macau', 'macdermot', 'macedonia', 'macedonian', 'machina', 'machine', 'machines', 'machu', 'mack', 'mackinnon', 'macklemore', \"macklemore's\", 'maclachlan', 'macos', 'macron', 'macy', 'mad', 'madagascar', 'madden', 'madding', 'made', 'maeda', 'magahi', 'magazine', \"magazine's\", 'maggie', 'magic', 'magical', 'magma', 'magnetized', 'maguire', 'mahal', 'maharashtra', 'maid', 'maiden', 'maidenhead', 'main', 'maine', 'mainland', 'mainland.', 'mainlands', 'mainly', 'mainstream', 'maintain', 'maintained', 'maintaining', 'major', 'majority', 'make', 'makemake', 'makes', 'making', 'malacostraca', 'maladie', 'malay', 'malayalam', 'maldives', 'male', 'males', 'malick', 'malik', 'malkovich', 'mall', 'malle', 'malta', 'malta,', 'maltese', 'maltz', 'mamata', 'mamluks', 'man', 'man,', 'managed', 'management', 'manager', 'managerial', 'managing', 'manchester', 'manchester,', 'mandy', 'manga', 'manhattan', 'manicured', 'manifestation', 'manifested', 'manner', 'mansion', 'mansions', 'mantee', 'mantegna', 'mantua', 'manual', 'manufactured', 'manufacturer', 'manufactures', 'manufacturing', 'manuscripts', 'many', 'maps', 'mar', 'maras', 'marathi', 'marble', 'marble,', 'marc', 'march', 'marches', 'marcomannic', 'marcus', 'mareros', 'maria', 'mariah', 'marie', 'marigold', 'marija', 'marine', 'mariner', 'marineris', 'marineris,', 'marisa', 'maritime', 'marius', 'mark', 'marked', 'market', 'marketed', 'markets', 'marking', 'markle', \"markle's\", 'marks', 'marksman', 'marlens', 'marlon', 'maroon', 'marriage', 'married', 'marrow', 'marry', 'mars', 'marseille', 'marshall', 'martha', 'marti', 'martial', 'martin', 'martins', 'marvel', 'mary', 'maryland', 'mask', 'masque', 'mass', 'massachusetts', 'massive', 'mastani', 'master', 'mastermind', 'match', 'matches', 'material', 'maternal', 'mates', 'math', 'mathematical', 'mathematician', 'mathematics', 'mathers', 'matoaka', 'matriculating', 'matriculation', 'matrix', 'matt', 'matter', 'matters', 'matthew', 'max', 'max:', 'maxim', 'maximum', 'maxine', 'may', 'maybach', 'mayen', 'mayers', 'mayor', 'mayoral', 'mayweather', 'maze', 'mc', 'mcavoy', 'mcbride', 'mccain', 'mccarthy', 'mccartney', 'mcclane', 'mcconaughey', 'mccoys', 'mcelhenney', 'mcewen', 'mcgee', 'mcguire', 'mckay', 'mclaren', 'mclaughlin', 'mcteigue', 'me', 'mead', 'meade', 'meagan', 'meals', 'mean', 'meaning', 'meant', 'meanwhile', 'measured', 'measurement', 'measurements', 'measures', 'meat', 'meaty', 'mechanics', 'medalist', 'medals', 'media', 'media,', 'median', 'medical', 'medication', 'medication,', 'medications', 'medicinal', 'medicine', 'medieval', 'mediterranean', 'medium', 'mediums', 'meet', 'meeting', 'meets', 'megalopolis', 'meghan', 'mehta', \"mehta's\", 'mei', 'meister', \"meister's\", 'melancholia', 'melbourne', 'melilla', 'melissa', 'melita', 'mellark', 'mellila', 'melodrama', 'melody', 'member', 'members', 'membership', 'membership.', 'memoir', 'memorial', 'memories', 'memory', 'men', \"men's\", 'men,', 'men:', 'menace', 'mendes', 'menon', 'mensa', \"mensa's\", 'mensah', 'mensch', 'mental', 'mention', 'mentioned', 'mentoring', 'mercedes', 'mercer', 'merciless', 'mercury', 'meredith', 'merged', 'merger', 'merill', 'meriwether', 'merrick', \"merrick's\", 'merton', 'meryl', 'messiah', 'messianism', 'met', 'metal', 'metallurgy', 'metals', 'metaphysics', 'meteora', 'meters', 'method', 'methodology', 'methods', 'metro', 'metropolitan', 'metv', 'mexican', 'mexicana', 'mexicans,', 'mexico', 'meyer', \"meyer's\", 'meyers', 'mi', 'mi6', 'miami', 'mian', 'mice', 'michael', 'michel', 'michelle', 'michigan', 'mick', 'micke', 'mickey', 'microscopic', 'microsoft', 'mid', 'middle', 'middlesex', 'midlands', 'midlands,', 'midnight', 'midrash', 'midtown', 'mifflin', 'might', 'migrated', 'migrations', 'mikaelson', 'mike', 'mikhail', 'milan', 'mild', 'mildew', 'miles', 'miley', 'milhous', 'militant', 'military', 'milk', 'milky', 'millar', 'millennia', 'millennials', 'millennium', 'miller', 'million', 'millionaire', 'millions', 'mills', 'milo', 'milwaukee', 'milwaukee,', 'mimic', 'min', 'mind', 'mineral', 'miners', 'mines', 'miney', 'mini', 'minimal', 'minimalist', 'mining', 'miniseries', 'minister', 'ministry', 'ministry,', 'minneapolis', 'minnesota', 'minnie', 'minogue', 'minor', 'minority', 'minsheng', 'minster', 'minutes', 'mira', 'miracle', 'miranda', 'mis', 'miscarried', 'mischief', 'miser', 'misleading', 'missile', 'mission', 'mississippi', 'missouri', 'mitch', 'mitchell', 'mitsuyo', 'mixed', 'mixtape', 'moan', 'mob', 'mobile', 'mobile,', 'mockingbird', 'mockingjay', 'mode', 'model', 'modeled', 'modelling', 'moderate', 'moderated', 'moderately', 'modern', 'modernity', 'modes', 'modification', 'modified', 'modular', 'moesia', 'mogul', 'mohabbatein', 'mohawk', 'moi', 'moiseiwitsch', 'molds', 'molecular', 'molecule', 'molly', 'mom', 'monaghan', 'monarch', 'monarchs', 'monarchy', 'monday', 'monday,', 'mondays', 'monde', 'monetary', 'money', 'moneyball', 'mongol', 'mongolia', 'monica', 'monica,', 'monique', 'monosodium', 'monotheistic', 'monroe', 'monroe,', 'mons', 'monster', 'monsters', 'montana', 'monteiro', 'montgomery', 'month', 'months', 'monument', \"monument's\", 'moon', 'moonraker', 'moore', 'moors', 'moral', 'moran', 'moray', 'more', 'morena', 'morgan', 'mori', 'moriah', 'mormon', 'morning', 'morocco', 'morris', 'morrison', 'mortally', 'mortgage', 'moscone', 'moscovium', \"moscovium's\", 'moss', 'most', 'mostly', 'motel', 'mother', \"mother's\", 'motion', 'motor', 'motorcycles', 'motors', \"motors'\", 'motown', 'mott', 'mount', 'mountain', 'mountainless', 'mountains', 'mountbatten', 'mourners', 'move', 'moved', 'movement', 'movements', 'moves', 'movie', \"movie's\", 'movie,', 'movies', 'moving', 'moyer', 'moynahan', 'mozart', \"mozart's\", 'mozilla', 'mp', 'mp3', 'mpaa', 'mr.', 'ms', 'msg', 'mtv', 'much', 'mucosa', 'mud', 'mughal', 'muhammad', 'mukerji', 'mukesh', 'mukherjee', 'mulder', 'mull', 'mullin', 'multi', 'multicellular', 'multinational', 'multiplayer', 'multiple', 'multiplication', 'multitasking', 'multnomah', 'mumbai', 'munich', 'municipal', 'municipalities', 'municipality', 'munitions', 'munroe,', 'mural', 'murder', 'murdered', 'murderer', 'murders', 'murdock', 'murphy', 'murrah', 'musala', 'musalla', 'muscle', 'museum', 'music', 'musical', 'musically', 'musician', 'musicians', 'muslim', 'mutant', 'mute', 'mutiny', 'muammad', 'mvd', 'my', 'mydland', 'mysterious', 'mystery', 'mystic', 'mysticism', 'myth', 'mythologies', 'mythology', 'myths', 'n', \"n'\", \"n't\", 'n.w.', 'naacp', 'naagin', 'naan', 'naked', 'name', 'named', 'nameless', 'names', 'nanotechnology', 'nanterre', \"naqi'a\", 'narrative', 'nasa', 'nasir', \"nasser's\", 'natasha', 'nate', 'nathan', 'nathanson', 'nation', 'national', 'nationale', 'nationalist', 'nationalists', 'nationality', 'nations', 'native', 'nativity', 'nato', 'natural', 'naturally', 'nature', 'nausea', 'naval', 'navarre', 'navies', 'navies,', 'navigation', 'navigator', \"navigator's\", 'navy', 'nawaz', 'nazario', 'nazi', 'nba', 'nbc', 'nc', 'ncaa', 'ne', 'nea', 'neal', 'near', 'nearby', 'nearly', 'nebraska', 'neck', 'nectar', 'ned', 'needs', 'neela', 'negate', 'negated', 'negative', 'negotiated', 'neighborhood', 'neighbourhoods', 'neil', 'neill', 'neither', 'nellie', 'nelson', 'neo', 'neon', 'nepal', 'nephew', 'neptune', 'neptunian', 'nero', \"nero's\", 'nervous', 'ner', 'ness', 'nest', 'netflix', 'netherlands', 'netscape', 'nettles', 'network', 'networking', 'networks', 'neuromodulation', 'neuromodulator', 'neuromodulators', 'neuron', 'neurons', 'neurotransmitter', 'neutrinos', 'neutrons', 'nevada', 'neve', 'never', 'nevertheless', 'neville', 'new', 'newest', 'newkirk', 'newly', 'newman', 'news', 'newspaper', \"newspaper's\", 'newton', 'next', 'nfl', 'ngo', 'nh', 'niblet', 'nice', 'nicene', 'nichols', 'nicholson', 'nick', 'nickelodeon', 'nickname', 'nicknamed', 'nicolas', 'nicole', 'nicotine', 'niece', 'night', 'nightmare', 'nights', 'nightshades', 'nikita', 'nikki', 'nina', 'nine', 'nineteen', 'nineteenth', 'nineties', 'ninety', 'ninja', 'nintendo', 'ninth', 'niro', 'nirvana', 'nitin', 'nixon', 'no', 'no.', 'noah', 'nobel', 'noble', 'nobody', 'noir', 'nokia', 'nol', 'nolan', 'nolte', 'nomadic', 'nominal', 'nominated', 'nomination', 'nominations', 'nominee', 'non', 'none', 'nonfiction', 'nonperforming', 'nonphysical', 'nonprofit', 'nonviolent', 'nor', 'nora', 'nordhoff', 'norepinephrine', 'norfolk', 'normal', 'normally', 'norman', 'north', 'northamptonshire', 'northeast', 'northeastern', 'northern', 'northwest', 'northwestern', 'norton', 'norway', 'norway,', 'norwegian', 'norwood', 'nostalgia', 'not', 'notable', 'notably', 'note', 'notes', 'nothing', 'notoriety', 'notorious', 'nottingham', 'nottingham,', 'novel', 'novelist', 'novella', 'novello', 'novels', 'november', 'now', 'nowhere', 'noxon', 'noyce', 'nri', 'ntv', 'nu', 'nuclear', 'nude', 'nukes', 'number', 'numbered', 'numbers', 'numenius', 'numerous', 'nust', 'nwa', 'nxt', 'nyc', 'nymph', 'nymphomaniac', 'n', 'ne', 'o', \"o'brien\", \"o'connor\", \"o'dea\", 'oakland', 'obama', \"obama's\", 'obata', 'obe', 'obelisk', 'obeyed', 'obfuscation', 'object', 'objects', 'obs', 'obscure', 'observable', 'observance', 'observe', 'obstacles', 'obtaining', 'obverse', 'obvious', 'occasion', 'occasional', 'occasionally', 'occasions', 'occult', 'occupation', 'occupying', 'occur', 'occurred', 'occurring', 'occurs', 'ocd', 'ocean', 'oceania', 'oceanographer', 'october', 'octone', 'odd', 'oder', 'odeya', 'odor', 'of', 'off', 'offence', 'offense', 'offer', 'offered', 'offers', 'office', 'officer', 'offices', 'official', 'officially', 'officio', 'offline', 'offscreen', 'often', 'ofwgkta', 'ogre', 'ohba', 'ohio', 'oil', \"oil's\", 'oklahoma', 'oklahoma,', 'old', 'older', 'oldest', 'oldsmobile', 'oliver', 'olivia', 'olsen', 'olympian', 'olympians', 'olympic', 'olympics', 'olympus', 'olympus,', 'oman', 'omri', 'on', 'once', 'oncoming', 'one', \"one's\", 'oneness', 'ones', 'onexim', 'ongoing', 'online', 'only', 'onscreen', 'onto', 'ooltah', 'opel', 'open', 'opened', 'opening', 'openly', 'opera', 'operas', 'operated', 'operates', 'operating', 'operation', 'operations', 'operator', 'opportunities', 'opportunity', 'oppose', 'opposed', 'opposite', 'opposites', 'opposition', 'opted', 'optical', 'optics', 'optional', 'opus', 'or', 'oracular', 'oral', 'orally', 'orange', 'oranges', 'orbit', 'orbiting', 'orci', 'order', 'oregon', 'organ', 'organised', 'organism', 'organismal', 'organisms', 'organization', 'organizations', 'organized', 'oriana', 'oriented', 'origin', 'original', 'originally', 'originals', 'originated', 'originating', 'orion', 'orizari', 'orleans', 'ormond', 'ornamental', 'orson', 'ortega', \"ortega's\", 'os', 'osborne', 'oscar', 'oscar,', 'oscars', 'oscillations', 'osmotic', 'other', 'others', 'otherwise', 'ottoman', 'ottomans', 'ounces', 'out', 'outbreak', 'outer', 'outfits', 'outlet', 'outlets', 'outpouring', 'outside', 'outstanding', 'ovaries', 'ovary', 'over', 'overall', 'overboard', 'overbrook', 'overdoses', 'overdrive', \"overdrive's\", 'overexposed', 'overlap', 'overlaps', 'oversaw', 'overseas', 'oversight', 'ovum', 'own', 'owned', 'owner', 'owning', 'owns', 'oxford', 'oxygen', 'p.', 'p.s', 'p.s.', 'paanch', 'pace', 'pacers', 'pacha', 'pacific', 'pacifist', 'pacino', 'pack', 'package', 'padma', 'padua', 'padukone', \"padukone's\", 'page', 'pages', 'paguroidea', 'paid', 'pain', 'painted', 'painter', 'painting', 'pairs', 'pakir', 'pakistan', \"pakistan's\", 'pakistani', 'palace', 'paleoconservative', 'palestinian', 'pallavi', 'palma', 'palme', 'palmer', 'pamela', 'panagia', 'panama', 'panasonic', 'pancrase', 'pancrustacea', 'panda', 'pandora', 'panellist', 'panic', 'pannonia', 'panorama', 'pantheon', 'panthera', 'papal', 'paper', 'parachuting', 'parachutist', 'parachutists', 'paradigms', 'paradise', 'paradise,', 'paramount', 'paranormal', 'paraphyletic', 'paratrooper', 'paratroopers', 'parent', 'parenthood', 'parents', 'paria', 'parindey', 'paris', 'paris,', 'parity', 'park', \"park's\", 'park,', 'parker', 'parking', \"parkinson's\", 'parkinsonism', 'parks', 'parliament', 'parliamentary', 'parlophone', 'parma', 'parnassus', 'parnelli', 'part', 'parthenon', 'partially', 'participant', 'participants', 'participate', 'participated', 'participates', 'participation', 'particles', 'particular', 'particularly', 'parties', 'partly', 'partner', 'partnership', 'parts', 'party', 'pasadena', 'pasadena,', 'passed', 'passenger', 'passing', 'passing,', 'passion', 'past', 'past,', 'pastoral', 'pat', 'patel', 'patents', 'path', 'patients', 'patna', 'patric', 'patricia', 'patrick', 'patriot', 'patriots', 'pattern', 'patterns', 'pattie', 'patty', 'paul', 'paula', 'pavitra', 'pawn', 'paxton', 'pay', 'payment', 'payphone', 'pays', 'pays,', 'pb', 'pbr', 'pbs', 'pd', 'pe', 'peabody', 'peace', 'peaceful', 'peak', 'peaked', 'peaks', 'pearl', 'peasant', 'peck', 'peele', 'peers', 'peeta', 'pegg', 'peipus', 'peloponnese', \"peloponnese's\", 'pembroke', 'pen', 'pendleton', 'penguin', 'penguins', 'peninsula', 'peninsulas', 'penn', 'pennsylvania', 'pentagon', 'penultimate', 'people', \"people's\", 'people,', 'peoples', 'peppers', 'per', 'perceived', 'percent', 'perceptual', 'percussion', 'perdition', 'perego', 'perfect', 'perform', 'performance', 'performance.roland', 'performances', 'performed', 'performer', 'performers', 'performing', 'performs', 'perfume', 'perhaps', 'period', 'periodically', 'periods', 'perished', 'perkins', 'perlman', 'permanent', 'permanently', 'permission', 'permit', 'perpetuated', 'perry', 'persephone', 'perseus', 'persia', 'persian', 'person', \"person's\", 'persona', 'personal', 'personality', 'personnel', 'persons', 'perspectives', 'peru', 'pet', 'peter', 'peterborough', 'petrie', 'petroleum', 'pets', 'pettiford', 'petyr', 'pfister', 'phaneritic', 'pharmaceuticals', 'pharmacology', 'phase', 'phenomena', 'phenomenon', 'phil', 'philadelphia', 'philanthropist', 'philanthropy', 'philip', 'philips', 'phillip', 'phillips', 'philosopher', 'philosophical', 'philosophies', 'philosophy', 'phinney', 'phoenix', 'phoenix,', 'phone', 'photo', 'photographed', 'photographer', 'photographers', 'photographs', 'photography', 'photoshop', 'phrase', 'physical', 'physically', 'physicians', 'physics', \"physics'\", 'physiological', 'piacenza', 'pianist', 'piano', 'picchu', 'picked', 'pico', 'picture', 'pictures', 'piece', 'pieces', 'pier', 'pierce', 'pierre', 'pig', 'piku', 'pileggi', 'pilgrimage', 'pilot', 'piloted', 'pilots', 'pink', \"pink's\", 'pinkett', 'pinnacle', 'pinsky', 'pioneering', 'pirates', 'pirlo', 'pitch', 'pitt', 'pittsburgh', 'pixar', 'pizzas', 'place', 'placed', 'placentia', 'places', 'plagiarizing', 'plain', 'plainfield', 'plainfield,', 'plan', 'plane', 'planes', 'planet', 'planets', 'planned', 'planning', 'plans', 'plantations', 'plants', 'plateau', 'platform', 'platformer', 'platforms', 'platinum', 'plato', 'play', 'playboy', 'played', 'player', 'player,', 'players', 'playing', 'playlist', 'playmate', 'playoff', 'plays', 'playwright', 'plea', 'pleasure', 'pled', 'pledges', 'plot', 'plumage', 'plumber', 'plus', 'plutarch', 'pluto', 'pm', 'pneumonia', 'po', 'pocahontas', \"pocahontas's\", 'pocket', 'pocock', 'podcast', 'podcasts', 'poe', 'poem', 'poetry', 'poignant', 'point', 'points', 'poisoning', 'pol', 'poland', 'polanski', 'polar', 'police', 'policies', 'policy', 'polish', 'polished', 'political', 'politically', 'politician', 'politicians', 'politics', 'poll', 'polymath', 'polynomial', 'polyus', 'pomerance', 'pontiac', 'pool', 'poorest', 'pop', 'popular', 'popularity', 'popularly', 'populated', 'population', 'populations', 'populous', 'porphyria', 'porphyritic', 'porphyry', 'port', 'ported', 'porthos', 'portion', 'portland', 'portland,', 'portray', 'portrayal', 'portrayed', 'portraying', 'portrays', 'portugal', 'portuguese', 'poseidon', 'position', 'positions', 'positive', 'possessed', 'possesses', 'possession', 'possessions', 'possible', 'possibly', 'post', 'posters', 'posthumous', 'posthumously', 'postmaster', 'potato', 'potentially', 'potter', 'poultry', 'pound', 'power', 'power,', 'powered', 'powerful', 'powerfully', 'powers', 'powhatan', 'ppas', 'practical', 'practice', 'practiced', 'practices', 'practicing', 'practitioner', 'pradeep', 'prague', \"prague's\", 'praise', 'praised', 'prawns', 'prayer', 'prc', 'pre', 'preceded', 'precious', 'predated', 'predating', 'predominantly', 'predominately', 'preeminent', 'prefecture', 'prefer', 'preferred', 'prefers', 'pregnancy', 'prejudice', 'premier', 'premiere', 'premiered', 'premiering', 'premise', 'prepare', 'prepares', 'prequel', 'prescribed', 'presence', 'present', 'presented', 'presenter', 'presenting', 'preserved', 'presidency', 'president', 'presidential', 'presidents', 'pressure', 'prestigious', \"prestigious'\", 'preston', 'prevent', 'prevents', 'previously', 'prew', \"prew's\", 'prey', 'price', 'prices', 'pride', 'primarily', 'primary', 'prime', 'primetime', 'prince', 'princess', 'princeton', 'principal', 'principalities', 'principally', 'principles', 'printed', 'printer', 'printing', 'prior', 'priority', 'prison', 'prisoner', 'prisoners', 'pritchett', 'pritzwalk', 'private', 'privately', 'privileged', 'prix', 'prize', 'prizes', 'pro', 'probation', 'problems', 'probot', 'procedural', 'procedures', 'procedures,', 'process', 'proclaimed', 'proclaiming', 'procreation', 'prodigious', 'produce', 'produced', 'producer', 'producer,', 'producer/director/writer', 'producers', 'produces', 'producing', 'product', 'production', 'productions', 'products', 'professed', 'profession', 'professional', 'professionally', 'professor', 'proficient', 'profit', 'program', 'programing', 'programme', 'programmer', 'programming', 'programs', 'programs/facilities', 'progression', 'prohibits', 'project', 'projecting', 'projects', 'prokhorov', 'prolific', 'prom', 'prominence', 'prominent', 'prominently', 'promote', 'promoted', 'promoter', 'promotes', 'promotion', 'promotions', 'proof', 'propaganda', 'propagandist', 'proper', 'properties', 'property', 'proponents', 'proposed', 'proprietary', 'prosperity', 'prosperous', 'protagonist', 'protector', 'protectorate', 'protects', 'protest', 'protestant', 'protestantism', 'protestants', 'prototype', 'proved', 'provide', 'provided', 'providence', 'provides', 'province', 'provinces', 'provisionally', 'prowess', 'proximity', 'pseudoscience', 'psych', 'psychic', 'psycho', 'psychoactive', 'psychological', 'psyduck', 'public', 'publication', 'publicly', 'publish', 'published', 'publisher', 'publishing', 'puerto', 'puff', 'pulmonary', 'pulp', 'punch', 'punic', 'punished', 'punisher', 'punjab', 'punjabi', 'punk', 'puppies', 'purchasing', 'purdue', 'purely', 'purity', 'purple', 'purpose', 'purposes', 'push', 'push.', 'pushes', 'pushing', 'put', 'putative', 'putin', 'putting', 'pyaar', 'pyrenees', 'pythagoras', \"pythagoras'\", 'pythagorean', 'pythagoreanism', 'pythagoreans', 'python', 'q', 'qatar', 'qin', 'qs', 'quaid', 'qualifications', 'qualified', 'qualify', 'qualifying', 'qualitatively', 'qualities', 'quality', 'quarterback', 'quay', 'quebec', 'queen', 'queens', 'quentin', 'quesada', 'questions', 'quick', 'quickly', 'quiet', 'quit', 'quotes', 'r', 'r.', 'r.c.', 'r.r.', 'raasleela', 'rabbit', 'race', 'racecar', 'races', 'rachel', 'racing', 'radcliffe', 'radiation', 'radio', 'radioactive', 'radnor', 'rado', 'rae', 'rafael', 'rag', 'raggedy', 'raghav', 'ragni', 'ragtime', 'ragtime,', 'rahul', 'raids', 'rail', 'railroads', 'raimi', 'rainforest', 'rainier', 'raised', 'raising', 'raj', 'raj,', 'raja', 'rajskub', 'rakim', 'ralph', 'ram', 'ramji', 'ran', 'ranch', 'ranches', 'randall', 'randi', 'randolph', 'randy', 'range', 'ranged', 'ranges', 'ranging', 'rank', 'ranked', 'ranking', 'rankings', 'rap', 'rape', 'rapid', 'rapidly', 'rapper', 'rapson', 'rarely', 'rash', 'rasheed', 'raspberry', 'raster', 'rate', 'rated', 'rates', 'rather', 'rating', 'ratings', 'ratna', 'ratner', 'ratzenberger', 'raven', 'rawson', 'ray', 'rca', 're', 'reach', 'reached', 'reaching', 'reactions', 'read', 'readaption', 'readers', 'reading', 'reagan', 'real', 'realism', 'reality', 'reanimation', 'rear', 'reasons', 'rebecca', 'rebel', 'reboot', 'reborn', 'rebounds', 'reburial', 'reccared', 'receipts', 'receive', 'received', 'receives', 'receiving', 'recent', 'recently', 'recession', 'recipe', 'recipient', 'recognisable', 'recognised', 'recognition', 'recognizable', 'recognize', 'recognized', 'recognizes', 'recollection', 'recommends', 'reconnects', 'reconvened', 'record', 'recorded', 'recording', 'recordings', 'recordist', 'records', 'recovered', 'recovery', 'recreational', 'recruited', 'recruits', 'recurrences', 'recurrent', 'red', 'reddish', 'redemption', 'redgrave', 'redmond', 'reds', 'reduces', 'reed', 'reelected', 'reese', 'refer', 'references', 'referendum', 'referred', 'refers', 'reflecting', 'reflects', 'reform', 'reformer', 'reforms', 'refractive', 'refrain', 'refrained', 'refurbished', 'refusal', 'refuse', 'refused', 'refuses', 'regained', 'regaining', 'regan', 'regarded', 'regards', 'regent', 'regiment', 'reginald', 'region', 'regional', 'regions', 'registered', 'regular', 'regulate', 'regulating', 'regulations', 'regulatory', 'rehab', 'reign', 'reigned', 'reinterred', 'reisfeld', 'reitman', 'reject', 'rejected', 'rejoining', 'related', 'relates', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relatives', 'relaunch', 'relaxation', 'release', 'released', 'releases', 'releasing', 'reliance', 'religion', 'religions', 'religious', 'reloaded', 'relocated', 'remain', 'remained', 'remaining', 'remains', 'remake', 'remarried', 'remembered', 'remix', 'remote', 'removed', 'renamed', 'renaming', 'render', 'renewed', 'renner', 'renowned', 'rental', 'reorganization', 'repeated', 'repetitive', 'replaced', 'replacement', 'replacing', 'replanted', 'replicate', 'reply', 'report', 'reported', 'reportedly', 'reporting', 'reports', 'representation', 'representative', 'representatives', 'represented', 'representing', 'represents', 'repressed', 'repression', 'reprints', 'reprisal', 'reprise', 'reprised', 'reprising', 'reproductive', 'republic', 'republic,', 'republican', 'republicans', 'republics', 'reputation', 'requirements', 'requires', 'reruns', 'rescue', 'rescued', 'research', 'research,', 'reserve', 'reserves', 'reside', 'resident', 'residents', 'resides', 'resign', 'resignation', 'resistance', 'resnick', 'resounding', 'resources', 'respect', 'respective', 'respectively', 'response', 'responses', 'responsibility', 'responsible', 'rest', 'restaurant', 'restricted', 'rests', 'result', 'resulted', 'resulting', 'results', 'retail', 'retailer', 'retain', 'retained', 'retaining', 'retired', 'retirement', 'retreated', 'retributions', 'retrieving', 'return', 'returned', 'returning', 'reunited', 'revah', 'revealed', 'revenge', 'revenue', 'reversed', 'review', 'reviewed', 'reviewed,', 'reviews', 'revisited', 'revival', 'revived', 'revolution', 'revolutionary', 'revolutionized', 'revolver', 'revolves', 'rewritten', 'rey', \"rey's\", 'reynolds', 'rhode', 'rhodes', 'rhyme', 'rhythm', 'rhythmic', 'rhythms', 'rhne', 'riaa', 'ricans', 'rice', 'rich', 'richard', 'richelle', 'rick', 'rickman', 'rico', 'ridden', 'ride', 'riding', 'ridley', 'rifle', 'right', 'rights', 'rigidity', 'rihanna', 'rila', 'riley', 'ring', 'ringwald', 'rio', 'rip', 'ripe', 'rise', 'rishi', 'rishta', 'rising', 'risk', 'rita', 'rituals', 'rival', 'rivalry', 'rivals', 'river', 'rivers', 'riz', 'rizwan', 'rnb', 'road', 'roadgames', 'roads', 'roadshow', 'roadside', 'roadways', 'roald', 'rob', 'robb', 'robbie', 'robert', 'roberto', 'roberts', 'robin', 'robinson', 'robot', 'robots', 'robson', 'roc', 'roche', 'rock', 'rocked', 'rockets', 'rocky', 'roderic', 'rodger', 'rodney', 'roeg', 'rogen', 'roger', 'role', 'roles', 'roller', 'rolling', 'roma', 'roman', 'romance', 'romances', 'romancing', 'romani', 'romania', 'romanian', 'romans', 'romantic', 'rome', 'romero', 'ron', 'ronald', 'ronaldinho', 'rookie', 'room', 'rooms', 'rooney', 'root', 'rooted', 'roots', 'rosanna', 'rose', 'roseanne', 'rosemarie', 'roses', 'roses,', 'ross', 'roth', 'rothschild', 'rough', 'round', 'rouse', 'route', 'routes', 'routh', 'row', 'row,', 'rowdy', 'rowe', 'rowlands', 'rowling', 'roy', 'royal', 'royalty', 'ru', 'rudd', 'rude', 'rudy', 'rule', 'ruled', 'ruler', 'rules', 'ruling', 'rumble', 'rumored', 'run', 'runaway', 'runner', \"runner's\", 'running', 'runtime', 'runtimes', 'rus', 'rush', 'rushdie', 'russell', 'russells', 'russia', \"russia's\", 'russia,', 'russian', 'russo', 'rutgers', 'ruth', 'rutherfurd', 'rv', 'rwanda', 'rwanda,', 'ryan', 'ryder', 'rye', 'rza', 's.', 's.h.i.e.l.d.', 'saab', 'saas', 'saba', 'sabbath', 'sabbir', 'sacre', 'sacred', 'sadistic', 'sadler', 'saf', 'safe', 'safety', 'sag', 'sahara', 'saheb', 'sai', 'said', 'saif', 'sailor', 'saint', 'saints', 'saldana', \"saldana's\", 'saldaa', 'sales', 'salesman', 'salinger', 'sally', 'salma', 'salmaan', 'salman', 'salt', 'salter', 'salute', 'salvador', 'salvatore', 'salzburg', 'sam', 'samantha', 'samarkand', 'same', 'sameer', 'samos', 'sampled', 'samsung', 'samuel', 'samurai', 'san', 'sanchez', 'sanctioned', 'sanctioning', 'sandler', 'sands', 'sandwich', 'sang', 'sanskrit', 'sant', 'santa', 'santorini', 'sapphire', 'sarah', 'sargent', 'sarkar', 'sarsgaard', 'sassanid', 'satake', 'satellite', 'satire', 'satirical', 'satran', 'saturday', 'saturn', 'saudi', 'sauron', 'sausage', 'savage', 'saving', 'savior', 'savory', 'saw', 'saxo', 'saxon', 'say', 'sbs', 'scandal', 'scandinavia', 'scandinavian', 'scaramouch', 'scaramouche', \"scaramouche's\", 'scaramucci', 'scaramuccia', 'scarlett', 'scathing', 'scene', 'scenes', 'schedule', 'scheduled', 'scheme', 'schenck', 'scher', 'schiller', 'schmidt', 'school', 'school,', 'schools', 'schopenhauer', 'schulze', 'schwartz', 'science', 'sciences', 'scientific', 'scientist', 'scientists', 'scientology', 'scodelario', 'scope', 'scored', 'scores', 'scotland', 'scotsman', 'scott', 'scottish', 'scout', 'scouting', 'scranton', 'scrapped', 'scream', 'screen', 'screened', 'screenplay', 'screenwriter', 'scripps', 'script', 'scripted', 'scripting', 'scripts', 'scriptural', 'scriptures', 'scriptwriter', 'scuderia', 'scully', 'sculptors', 'sea', 'seabirds', 'seacrest', 'seal', 'seaman', 'seamen', 'sean', 'seaport', 'search', 'searching', 'season', 'season,', 'seasons', 'seasosn', 'seat', 'seater', 'seaters', 'seattle', 'sebastian', 'sec', 'second', 'secondhand', 'secret', 'secretary', \"secretary's\", 'secrets', 'section', 'sector', 'sectors', 'secularist', 'secure', 'secured', 'securing', 'sedative', 'sedgwick', 'see', 'see.sza.run', 'seemingly', 'seems', 'seen', 'segall', \"segall's\", 'segregationist', 'seibert', 'seine', 'seizure', 'sejong', 'sekhar', 'selected', 'self', 'selling', 'sells', 'selma', 'semi', 'semitic', 'senate', 'senator', 'senatorial', 'senators', 'sending', 'sends', 'senegalese', 'senior', 'sennacherib', \"sennacherib's\", 'sensation', 'sensations', 'sensitive', 'sensory', 'sentence', 'sentimental', 'seoul', 'separate', 'separated', 'separatist', 'september', 'sequel', 'sequels', 'sequences', 'sequential', 'serbia', 'serbian', 'seretse', 'serial', 'serialized', 'series', \"series'\", \"series's\", 'serious', 'serotonin', 'servant', 'serve', 'served', 'server', 'serves', 'service', 'serviced', 'services', 'serving', 'session', 'set', 'setbacks', 'seth', 'setting', 'settle', 'settled', 'settlement', 'settlers', 'setup', 'seven', 'seventh', 'seventies', 'several', 'severe', 'severely', 'severn', 'seville', 'sex', 'sexiest', 'sexual', 'sexually', 'seymour', 'shadowhunters', 'shaffir', 'shaft', 'shake', 'shakespeare', \"shakespeare's\", 'shaking', 'shakira', 'shallow', 'shampoo', 'shandong', 'shane', 'shang', 'shannon', 'shape', 'share', 'shared', 'shares', 'sharif', 'sharing', 'shark', 'sharks', 'shawshank', 'she', 'sheedy', 'sheen', 'sheet', 'shehbaz', 'shell', 'shellback', 'shelley', 'shenandoah', 'shepard', 'shepherd', 'sheridan', 'sherilyn', 'sherriff', 'sheryl', \"shi'a\", 'shifts', 'shikoku', 'shinsegae', 'shipbuilding', 'shipments', 'ships', 'shipwrecked', 'shirt', 'shnitzel', 'shobha', 'shoelace', 'shoes', 'shomu', 'shooter', 'shooting', 'shop', 'shops', 'short', 'shorted', 'shortened', 'shorter', 'shortest', 'shortly', 'shortness', 'shot', 'shoulders', 'show', 'show,', 'showed', 'showing', 'shown', 'shows', 'showtime', 'showtimes', 'shrek', 'shri', 'shrimp', 'shruti', 'shunned', 'shuster', 'shut', 'sibling', 'side', 'sidney', 'sidonie', 'siege', 'siegel', 'sierra', 'sight', 'sign', 'signage', 'signature', 'signed', 'significant', 'significantly', 'signing', 'signs', 'sihung', 'silence', 'silent', 'silesia', 'silver', 'simi', 'similar', 'similarities', 'simmons', 'simon', 'simple', 'simply', 'simulate', 'simultaneously', 'sinaloa', 'since', 'sinclair', 'sing', 'singapore', 'singer', 'singers', 'singh', 'singing', 'single', 'single,', 'singles', 'sings', 'sink', 'sint', 'sir', 'sire', 'sirens', 'sister', 'sisters', \"sisters'\", 'sitcom', 'sitcoms', 'site', 'sites', 'sits', 'situated', 'situation', 'situational', 'situations', 'six', 'sixteen', 'sixth', 'sizable', 'size', 'sized', 'skarsgrd', 'skateboarding', 'sketch', 'skiles', 'skills', 'skin', \"skin's\", 'skinner', 'skins', 'skip', 'skipped', 'skirmisher', 'skopje', 'skunk', 'sky', 'skyfall', 'skynyrd', \"skynyrd's\", 'skyscraper', 'slam', 'slasher', 'slater', 'slaughterhouses', 'slavery', 'slayer', \"slayer's\", 'sledgehammer', 'sleepers', 'slight', 'slimane', 'slime', 'slow', 'slowly', 'slowness', 'slumdog', 'smackdown', 'small', 'smallest', 'smart', 'smartphone', 'smartphones', 'smith', 'smoking', 'smriti', 'smritis', 'smrti', 'smyrna', 'smyth', 'snail', 'snake', 'snakes', 'snatcher', 'sniper', 'snow', 'snyder', 'so', 'soap', 'soapstone', 'soared', 'soccer', 'social', 'social,', 'socialist', 'socialists', 'society', 'society,', 'socio', 'soderbergh', 'software', 'sol', 'solanum', 'solar', 'sold', 'soldier', 'soldiers', 'sole', 'solely', 'solemn', 'solicitor', 'solid', 'solidification', 'solo', 'solomon', 'solutions', 'solve', 'somali', 'some', 'some,', 'someone', \"someone's\", 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'son,', 'song', 'songs', 'songwriter', 'songwriting', 'sonny', 'sons', 'sony', 'soon', 'sophie', 'sophie,', 'sophomore', 'sorcery', 'soshihiro', 'sought', 'soul', 'soul,', 'souls', 'sound', 'soundtrack', 'soup', 'soups', 'source', 'south', 'southcliffe', 'southeast', 'southeastern', 'southern', 'southwest', 'southwestern', 'sovereign', 'sovereignty', 'soviet', 'soybeans', 'space', 'spader', 'spain', \"spain's\", 'spanish', 'spans', 'spark', 'sparked', 'sparking', 'sparkling', 'spawned', 'speak', 'speaker', 'speakers', 'speaking', 'spears', \"spears's\", 'special', 'specialized', 'specializing', 'specially', 'specials', 'speciation', 'species', 'specific', 'specifically', 'spectator', 'spector', 'spectre', 'spectrum', 'speech', 'spencer', 'spend', 'spends', 'spent', 'spherical', 'spider', 'spielberg', 'spin', 'spirit', 'spirits', 'spiritual', 'spite', 'splinter', 'split', 'spoke', 'spoken', 'spokesman', 'spongebob', 'sponsorship', 'spoof', 'spoons', 'sport', 'sporting', 'sports', 'sportswear', 'spots', 'spread', 'spreading', 'spring', 'springfield,', 'spy', 'sq', 'sqmi', 'square', 'squarepants', 'squid', 'squids', 'squidward', 'sreenu', 'sri', 'sruti', 'sse', 'ssr', 'sssbs', 'st', 'st.', 'stadium', 'stadium,', 'stadiums', 'staff', 'stage', 'staged', 'staging', 'stainless', 'stalker', 'stalks', 'stamp', 'stance', 'stand', 'standard', 'standardized', 'standards', 'standing', 'stanfield', 'stanford', 'stanley', 'stanwix', 'staples', 'star', 'stargate', 'stark', \"stark's\", 'starred', 'starring', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'statements', 'states', \"states'\", 'statesman', 'static', 'stationary', 'statistical', 'staton', 'statue', 'status', 'statute', 'statutory', 'stay', 'stayed', 'stead', 'steak', 'steal', 'steals', 'steamship', 'steel', 'steelers', 'steeped', 'stella', 'step', 'stephanie', 'stephen', 'stephenie', 'stereotypical', 'steve', 'steven', 'stevenson', 'stewart', 'stews', 'sticker', 'stiles', 'still', 'stiller', 'stimulant', 'stimulated', 'stirling', 'stock', 'stocks', 'stomp', 'stone', \"stone's\", 'stonem', 'stop', 'stopped', 'storage', 'store', 'stored', 'stores', 'stories', 'storm', 'stormtrooper', 'stormzy', 'story', 'storyboard', 'storytellers', 'storytelling', 'strahovski', \"strahovski's\", 'straight', 'strait', 'stranger', 'strapped', 'strategic', 'strategically', 'strathairn', 'stratovolcanic', 'stratovolcano', 'streak', 'stream', 'streaming', 'streckfus', 'streep', 'street', 'strength', 'strengthened', 'strep', 'stress', 'stretches', 'stretching', 'strict', 'strictly', 'strikeforce', 'strings', 'strip', 'stripes', 'stripped', 'strode', 'strokes', 'strong', 'stronger', 'strongest', 'strongly', 'structure', 'struggle', 'struggles', 'struggling', 'strzechowski', 'stuart', 'student', 'students', 'studied', 'studies', 'studio', 'studios', 'study', 'sturges', 'style', 'styled', 'styles', \"styles's\", 'styling', 'stylised', 'sub', 'subatomic', 'subclass', 'subcommittee', 'subcontinent', 'subcultural', 'subdivided', 'subduction', 'subgenre', 'subject', 'subjective', 'subjects', 'submarine', 'submit', 'subordinated', 'subphylum', 'subsequent', 'subsequently', 'subset', 'subsidiary', 'subsists', 'subspecies', 'substance', 'substantial', 'subtraction', 'suburb', 'suburban', 'succeeded', 'success', 'successful', 'successfully', 'succession', 'successive', 'successor', 'such', 'sud', 'suddenly', 'sue', 'suffered', 'suffixes', 'suffolk', 'sugar', 'sugarland', 'suggested', 'suicide', 'suit', 'suite', 'sullenberger', 'sulley', 'sultanate', 'sumerians', 'summaries', 'summer', 'summerslam', 'summerslam,', 'summertime', 'summit', 'sumter', 'sun', 'sunburn', 'sundance', 'sunday', 'sung', 'sungard', 'sunny', 'super', 'superbad', 'superdome', 'superfamily', 'superheavy', 'superhero', 'superheroes', 'superheroes,', 'superhuman', 'superior', 'superman', 'supernatural', 'supervillain', 'supervision', 'supervisor', 'supervisors', 'supplemental', 'supplied', 'supply', 'support', 'supported', 'supporter', 'supporters', 'supporting', 'supports', 'supposed', 'supposedly', 'suppress', 'supreme', 'sur', 'surf', 'surface', 'surfer', 'surgeons', 'surpassed', 'surrender', 'surrendered', 'surrenders', 'surrey', 'surrounded', 'surrounding', 'surrounds', 'surveillance', 'survey', 'survival', 'survived', 'surviving', 'survivor', 'susan', 'suspended', 'sustain', 'sutherland', 'suzanne', 'svalbard', 'swamp', 'swayze', 'sweatshirt', 'swede', 'sweden', 'sweden,', 'swedish', 'sweet', 'swift', 'swim', 'swimmers', 'swiss', 'switch', 'switched', 'switzerland', 'swofford', \"swofford's\", 'sword', 'swords', 'syco', 'sydney', 'sydney,', 'syfy', 'sylvain', 'symbol', 'sympathizer', 'symptom', 'symptoms', 'syndicate', 'syndication', 'syndrome', 'synecdoche', 'synecdoche,', 'synonyms', 'synthesized', 'synthetic', 'synthpop', 'syria', 'syrian', 'system', 'system,', 'systems', 'sza', \"sza's\", 'snchez', 't', \"t'challa\", \"t's\", 't.', 'taarak', 'tacoma', 'tactics', 'taejong', 'taft', \"taft's\", 'tag', 'tagalog', 'tagline', 'tahoma', 'tail', 'taipei', 'taiwan', 'taiwanese', 'taj', 'tajikistan', 'tak', 'take', 'taken', 'takeo', 'takes', 'takeshi', 'taking', 'takings', 'tale', 'talent', 'talented', 'tales', 'talk', 'tall', 'tallest', 'talmud', 'talulah', 'tambor', 'tamerlane', 'tamil', 'tampa,', 'tandem', 'tang', 'tangled', 'tankers', 'tanya', 'tao', 'taoist', 'tape', 'taped', 'tarantino', 'targaryen', \"targaryen's\", 'target', 'tariffs', 'tartakovsky', 'tasks', 'tasman', 'taste', 'tatchell', 'tate', 'taught', 'tavern', 'taxon', 'taxonomist', 'taxonomy', 'taylor', 'tbs', 'teacher', 'teaches', 'teaching', 'teachings', 'team', 'teams', 'tech', 'technical', 'technicolor', 'techniques', 'techniscope', 'technologically', 'technologies', 'technology', 'tectonic', 'ted', 'teen', 'teenager', 'teeth', 'teflon', 'tej', 'tele', 'telecommunications', 'telefilms', 'telephone', 'televised', 'television', 'teller', 'tells', 'telly', 'telugu', 'temperatures', 'tempest', 'templar', 'temple', 'temporarily', 'temporary', 'temr', 'temr', 'ten', 'tenacious', 'tend', 'tends', 'tennant', 'tennessee', 'tennis', 'tension', 'tensions', 'tent', 'tentacles', 'tenth', 'tenure', 'terabithia', 'tere', 'terence', 'teresa', 'term', 'termed', 'terminals', 'terminate', 'terminating', 'terms', 'terrain', 'terrence', 'terrestrials', 'terri', 'terrible', 'territorial', 'territories', 'territory', 'terror', 'terrorist', 'tertiary', 'test', 'testes', 'testing', 'texas', 'textbook', 'textiles', 'texts', 'textual', 'texture', 'textures', 'tf2', 'tgi', 'than', 'thane', 'thanks', 'thanksgiving', 'that', \"that's\", 'the', 'theater', \"theater's\", 'theaters', 'theatre', 'theatres', 'theatrical', 'theatrically', 'their', 'them', 'theme', 'themed', 'themes', 'themselves', 'then', 'theo', 'theodore', 'theology', 'theophilus', 'theoretical', 'theorist', 'theory', 'thera', 'therapeutic', 'therapist', 'therasia', 'there', 'thereafter', 'thereby', 'therefore', 'theresa', \"theresa's\", 'thermodynamics', 'these', 'thespian', 'they', 'thi', 'thiam', 'thieves', 'thin', 'thing', 'things', 'thinking', 'thinnest', 'thirasa', 'third', 'thirteen', 'thirteenth', 'thirtieth', 'thirty', 'this', 'thomas', 'thompson', 'thoracic', 'those', 'thou', 'though', 'thought', 'thoughts,', 'thousand', 'thousands', 'three', 'thrift', 'thriller', 'thrillers', 'throat', 'throne', 'thrones', 'thrones,', 'through', 'throughout', 'thrown', 'thrust', 'thunder', 'thundershower', 'thunderstorm', 'thunderstorms', 'thurber', 'thuringia', 'thursday', 'thursdays', 'thus', 'thwaites', 'thyra', 'tiberius', 'ticketmaster', 'tics', 'tidewater', 'tie', 'tied', 'tier', 'tiernan', 'ties', 'tiffany', 'tiger', 'tigers', 'tigre', 'tigrinya', 'tigris', 'tim', 'timberlake', 'time', 'time,', 'time.', 'timeframe', 'times', 'timor', 'timothy', 'timur', 'timurid', 'tin', 'tiny', 'titaness', 'titans', 'tithe', 'tithing', 'title', 'titled', 'titles', 'titlists', 'titular', 'tna', 'tnt', 'to', 'tobacco', 'tobago', 'tobey', 'today', 'together', 'tolkein', 'tolkien', 'tom', 'tomato', 'tomatoes', 'tomboy', 'tomei', 'tomorrow', 'tonight', 'tony', 'too', 'took', 'top', 'topics', 'topped', 'topping', 'tori', 'torn', 'tornadoes', 'toro', 'toronto', 'toshiba', 'total', 'totality', 'touch', 'touched', 'tour', 'toured', 'tourette', \"tourette's\", 'touring', 'tourism', 'tourist', 'tourists', 'tournament', 'touted', 'toward', 'towards', 'tower', 'town', 'towne', 'towns', 'toy', 'toys', 'trace', 'tracey', 'trachtenberg', 'track', 'tracking', 'tracks', 'tracy', 'trade', 'traded', 'trader', 'trading', 'tradition', 'traditional', 'traditionally', 'traditions', 'trafford', 'trafford,', 'tragedies', 'tragedy', 'tragic', 'tragicomedy', 'train', 'trained', 'training', 'trans', 'transactinide', 'transactions', 'transatlantic', 'transformation', 'transformations', 'transformed', 'transit', 'transition', 'transitioned', 'transits', 'translates', 'translation', 'translator', 'transliterated', 'transmitted', 'transoxiana', 'transport', 'transportation', 'transports', 'transylvania', 'trauma', 'travel', 'traveler', 'traveling', 'travelled', 'travelling', 'travels', 'traverse', 'traversing', 'travolta', 'treasure', 'treat', 'treated', 'treating', 'treatment', 'treats', 'treaty', 'trebbia', \"trebbia's\", 'trebia', 'tree', 'trees', 'trek', 'tremor', 'tremors', 'trenton', 'treves', 'trevor', 'tribal', 'tribe', 'tribes', 'tribune', 'tributary', 'trick', 'tried', 'trier', 'triggered', 'trilogy', 'trinian', \"trinian's\", 'trinidad', 'trinket', 'trinomen', 'trip', 'triple', 'triumphs', 'trivia', 'trojans', 'troops', 'trophies', 'trophy', 'tropical', 'tropics', 'trouble', 'troubled', 'troubles', 'troublesome', 'truck', 'trucks', 'true', 'trueba', 'truman', 'trump', 'trunk', 'try', 'tsenacommacah', 'tsugumi', 'tucci', 'tucker', 'tudor', 'tuesdays', 'tuft', 'tull', 'tundra', 'tuning', 'tunnel', 'tunnels', 'tupac', 'turbomachinery', 'turco', 'turkey', 'turkish', 'turks', 'turmoil', 'turn', 'turned', 'turner', 'turnips', 'turnpike', 'turturro', 'tuscaloosa', 'tutsi', 'tuxedo', 'tv', 'twain', \"twain's\", 'tweets', 'twelve', 'twenty', 'twice', 'twilight', 'twin', 'twitching', 'twitter', 'two', 'tycoon', 'tye', 'tyler', 'tyler,', 'type', 'types', 'typical', 'typically', 'tyrin', 'tyrion', 'u', 'u.s', 'u.s.', 'uae', 'ubiquitous', 'ud', 'ufc', 'ufficiale', 'uganda', 'ugliest', 'ugly', 'ugric', 'uk', 'ukraine', 'ukrainian', 'ultimate', 'ultimately', 'ultron', 'ulysses', 'umami', 'un', 'unable', 'unacquainted', 'unadapted', 'unadjusted', 'unaffected', 'unaffiliated', 'unarticulated', 'unassociated', 'unavailable', 'unaware', 'unbranded', 'unbreakable', 'uncle', 'uncommon', 'unconnected', 'unconsciousness', 'uncovered', 'uncovering', 'uncredited', 'und', 'undead', 'undefeated', 'under', 'undercover', 'underdog', 'underside', 'understand', 'understanding', 'understood', 'underway', 'undirected', 'undisputed', 'unemployed', 'unesco', \"unesco's\", 'unfair', 'unfamiliar', 'unfavorable', 'unfunded', 'unger', 'ungoverned', 'uni', 'unicef', 'unidentified', \"unido's\", 'unified', 'uniform', 'uniformed', 'uninsulated', 'uninterested', 'uninvolved', 'union', 'unique', 'unison', 'unit', 'unitary', 'united', \"united's\", 'unites', 'units', 'unity', 'universal', 'universe', 'universities', 'university', \"university's\", 'unknown', 'unlawful', 'unmarried', 'unofficial', 'unofficially', 'unprovoked', 'unqualified', 'unranked', 'unreachable', 'unrecognized', 'unrecorded', 'unrelated', 'unreleased', 'unremarkable', 'unsatisfactory', 'unscripted', 'unsuccessful', 'unsupervised', 'until', 'untouchables', 'untreated', 'unused', 'unusual', 'unvoiced', 'unwritten', 'up', 'upcoming', 'upheaval', 'upon', 'uprising', 'uptight', 'uptown', 'ural', 'uranium', \"uranium's\", 'uranus', 'urban', 'us', 'usa', 'usage', 'use', 'use,', 'used', 'useful', 'user', 'users', 'uses', 'using', 'ustad', 'usually', 'uta', 'utah', 'uzbekistan', 'v', 'vacate', 'vaccine', 'vacuum', 'vadim', 'vaitla', 'valarie', 'valiant', 'validity', 'valles', 'valley', 'valley,', 'valuable', 'value', 'valued', 'values', 'valve', 'vampire', 'vampires', 'van', 'vandals', 'vanessa', 'vanguard', 'varanasi', 'variable', 'variables', 'variably', 'variation', 'varied', 'variety', 'various', 'varun', 'vary', 'vasco', 'vast', 'vatican', 'vaughn', \"vaughn's\", 'vauxhall', 'vedas', 'vedic', 'veep', 'vegas', 'vegetables', 'vehicle', 'vehicles', 'venerated', 'veneto', 'venezuela', 'vengeance', 'ventura', 'venture', 'ventures', 'venus', 'vera', 'vere', 'vermont', 'vernon', 'veronica', 'versa', 'verse', 'version', 'versions', 'vertebrate', 'vertical', 'very', 'vessel', 'vestige', 'veteran', 'vettel', 'vh1', 'vi', \"vi's\", 'via', 'viacom', 'vic', 'vice', 'vickie', 'victim', 'victor', 'victoria', 'victories', 'victory', 'video', 'videos', 'videotape', 'vienna', 'vietnamese', 'view', 'viewed', 'viewers', 'viewership', 'views', 'vigilante', 'vii', 'viii', 'village', 'villain', 'vince', 'vincent', 'vinci', 'violence', 'violent', 'vip', 'virgin', 'virginia', \"virginia's\", 'virginian', 'visayas', 'viscount', 'visigoths', 'vision', 'visit', 'visited', 'visitors', 'visits', 'vista', 'vistula', 'visual', 'vital', 'vittorio', 'vladimir', 'vocal', 'vocalist', \"vocalist's\", 'vocalists', 'vocals', 'voice', 'voiced', 'voices', 'void', 'volcanic', 'volcano', 'volcanoes', 'volga', 'volkswagen', 'volleyball', 'voluntarily', 'volunteer', 'volunteers', 'vomiting', 'von', 'vote', 'voted', 'voting', 'voyage', 'vs.', 'w.', 'wadadli', 'wade', 'wahlberg', 'wahunsenacah', 'wahunsenacawh', 'wait', 'wakefulness', 'waladli', 'wales', 'walesa', 'walker', 'walking', 'wall', 'wallace', 'walled', 'wally', 'walsh', 'walt', 'walter', 'waluigi', 'wanda', 'wang', \"wang's\", 'want', 'wanted', 'wants', 'war', 'warcraft', 'ward', 'wards', 'warehouse', 'warenne', 'warfare', 'warner', 'warren', 'warrens', \"warrens'\", 'warring', 'warrior', 'warriors', 'wars', 'wars:', 'wartime', 'was', 'washed', 'washington', 'washington,', \"wasn't\", 'waste', 'watanabe', 'watched', 'watchmen', 'water', 'waters', 'waterspouts', 'watertown', 'watertown,', 'waterworld', 'watson', 'wave', 'wax', 'way', 'ways', 'wb', 'wealth', 'wealthy', 'weapon', 'weapons', 'wear', 'wearing', 'wears', 'weather', 'web', 'webber', 'weber', 'webisode', 'website', 'wed', 'wedding', 'wednesday', 'week', 'weekend', 'weekend,', 'weekly', 'weeknd', 'weeks', 'weeks,', 'weight', 'weights', 'weill', 'weimar', 'weird', 'welcomed', 'well', 'welles', \"welles's\", 'wells', 'welnick', 'wen', 'wendy', \"wendy's\", 'went', 'were', 'werewolf', 'wes', 'wessex', 'wesson', 'west', 'western', 'westerner', 'westeros', 'westminster', 'westminster,', 'westworld', \"westworld's\", 'what', 'whatsoever', 'whedon', 'wheeled', 'when', 'where', 'where,', 'whereas', 'wherein', 'which', 'while', 'whilst', 'whispering', 'white', 'whitehall', 'who', 'whole', 'wholly', 'whom', 'whoopi', 'whose', 'why', 'wide', 'widebody', 'widely', 'wider', 'widespread', 'widow', 'widows', 'wife', 'wii', 'wiig', 'wikipedia', 'wild', 'wildly', 'wilfred', 'wilhelm', 'wilkes', 'will', 'william', 'williams', 'williamson', 'willing', 'willis', 'wilson', 'wilsonianism', 'wilsonianism.', 'wiltshire', 'wimbledon', 'win', 'winchester', 'wind', 'wind.', 'window', 'windows', 'winds', 'windsor', 'windstorm', 'windstorms', 'winehouse', \"winehouse's\", 'wing', 'winner', 'winners', 'winning', 'winona', 'wins', 'winstead', 'winter', 'winterfell', 'wire', 'wireless', 'wisconsin', 'wisdom', 'wish', 'witch', 'with', 'withdrew', 'witherspoon', 'withholds', 'within', 'without', 'wives', 'wiz', 'wolf', 'wolfenbttel', 'wolfgang', 'wolfgangus', 'woll', 'wolsey', 'wolves', 'woman', 'womb', 'women', 'won', \"won't\", 'wonder', 'wood', 'wooden', 'woodlice', 'woodrow', 'woods', 'woodsman', 'woody', 'worcester', 'word', 'worden', 'wording', 'words', 'work', 'worked', 'worker', 'working', 'works', 'world', \"world's\", 'world,', 'worldwide', 'worldwide,', 'worn', 'worship', 'worshipped', 'worst', 'worth', 'would', \"wouldn't\", 'wounded', 'wounds', 'wresting', 'wrestled', 'wrestlemania', 'wrestler', 'wrestlers', 'wrestling', 'wright', 'write', 'writer', 'writer,', 'writers', 'writes', 'writing', 'writings', 'written', 'wrote', 'wu', 'wuthering', 'wwe', \"wy'east\", 'wyler', 'wylie', 'x', 'xbox', 'xiv', 'xlv', 'xxx', 'xxx:', 'y', \"y's\", 'yadira', 'yajna', 'yang', 'yano', 'yanyuhang', 'yard', 'ye', 'yeah', 'year', 'yearly', 'years', 'years,', 'yeh', 'yelich', 'yellow', 'yemen', \"yemen's\", 'yet', 'yin', 'yo', 'yoga', 'york', \"york's\", 'yorker', 'yorkville', 'you', 'young', \"young's\", 'younger', 'youngest', 'your', 'youth', 'youtube', 'ysl', 'yu', 'yugoslavia', 'yumi', 'yves', 'yvonne', 'zac', 'zachary', 'zack', 'zant', 'zara', 'zathura', 'zealand', \"zealand's\", 'zebra', 'zemechkis', 'zemeckis', 'zero', 'zetas', 'zeus', 'zindagii', 'ziyad', 'zmax', 'zodiac', 'zoe', 'zoey', 'zombie', \"zombie's\", 'zombies', 'zone', 'zones', 'zoo', 'zoroastrianism', \"zoroastrianism's\", 'zo', 'zrich', '~', '', '', 'land', 'cole', 'scar', 'uto', '', '', '', '', '', '', '', '', '', '', '', 'riq', 'ahr', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kxTuGGvotsf"
      },
      "source": [
        "### Padding for x and computation of max sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1uKddQlsCRR",
        "outputId": "493efe90-fcd3-41a7-d966-96115ffba5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def convert_text(df, tokenizer, is_training=False, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Converts input text sequences using a given tokenizer\n",
        "\n",
        "    :param texts: either a list or numpy ndarray of strings\n",
        "    :tokenizer: an instantiated tokenizer\n",
        "    :is_training: whether input texts are from the training split or not\n",
        "    :max_seq_length: the max token sequence previously computed with\n",
        "    training texts.\n",
        "\n",
        "    :return\n",
        "        text_ids: a nested list on token indices\n",
        "        max_seq_length: the max token sequence previously computed with\n",
        "        training texts.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    text_ids_claim = tokenizer.convert_tokens_to_ids(df['Claim'])\n",
        "    text_ids_evidence = tokenizer.convert_tokens_to_ids(df['Evidence'])\n",
        "\n",
        "    # Padding\n",
        "    if is_training:\n",
        "        max_seq_length_claim = int(np.quantile([len(seq) for seq in text_ids_claim], 0.99))\n",
        "        max_seq_length_evidence = int(np.quantile([len(seq) for seq in text_ids_evidence], 0.99))\n",
        "\n",
        "        if max_seq_length_claim > max_seq_length_evidence:\n",
        "            max_seq_length = max_seq_length_claim\n",
        "        else:\n",
        "          max_seq_length = max_seq_length_evidence\n",
        "\n",
        "    else:\n",
        "        assert max_seq_length is not None\n",
        "\n",
        "    df['Claim'] = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_claim]\n",
        "    df['Claim'] = np.array([seq[:max_seq_length] for seq in df['Claim']])\n",
        "    \n",
        "    df['Evidence'] = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_evidence]\n",
        "    df['Evidence'] = np.array([seq[:max_seq_length] for seq in df['Evidence']])\n",
        "\n",
        "    return max_seq_length\n",
        "        \n",
        "\n",
        "max_seq_length = convert_text(X_train, tokenizer, True)\n",
        "print(\"Max token sequence: {}\".format(max_seq_length))\n",
        "print('X train shape: ', X_train.shape)\n",
        "\n",
        "convert_text(X_val, tokenizer, False, max_seq_length)\n",
        "print('X val shape: ', X_val.shape)\n",
        "\n",
        "convert_text(X_test, tokenizer, False, max_seq_length)\n",
        "print('X test shape: ', X_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token sequence: 66\n",
            "X train shape:  (3594, 2)\n",
            "X val shape:  (2876, 2)\n",
            "X test shape:  (719, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhi2js_i6x0Z"
      },
      "source": [
        "## Sentence embedding\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Ed3CA5FUkM"
      },
      "source": [
        "embedding_vector_length = embedding_dimension"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyF5m7TP5nD7"
      },
      "source": [
        "def firstModel(embedding_vector_length,dim, start_lr=0.001):\n",
        "\n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True)(input)\n",
        "  last_state = SimpleRNN(dim,return_state=True)(x)   #can we add a Bidirectional layer??\n",
        "  \n",
        "  RNN = Model(input, last_state, name=\"firstModel\")\n",
        "\n",
        "\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  RNN.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "  return RNN\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example to check if it is working, no train done\n",
        "firstModel = firstModel(embedding_vector_length,32)\n",
        "firstModel.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = firstModel.predict(x_train[0])\n",
        "print(X_train.Claim[0])\n",
        "print(ris[0][1])"
      ],
      "metadata": {
        "id": "3Fc8SKdUZXHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEfIzk7vFNcd"
      },
      "source": [
        "def secondModel(embedding_vector_length,dim, start_lr=0.001):\n",
        "  \n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True)(input)\n",
        "  states = SimpleRNN(dim,return_sequences=True)(x)   #can we add a Bidirectional layer??\n",
        "  output = GlobalAveragePooling1D()(states)\n",
        "  RNN = Model(input, output, name=\"secondModel\")\n",
        "  \n",
        "  \n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  RNN.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "  return RNN\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "secondModel = secondModel(embedding_vector_length,32)\n",
        "secondModel.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = secondModel.predict(x_train[0])\n",
        "print(X_train.Claim[0])\n",
        "print(ris[0])"
      ],
      "metadata": {
        "id": "u1M24UjOZZHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7u666kwGsqG"
      },
      "source": [
        "def thirdModel(embedding_vector_length, dim, start_lr=0.001):\n",
        "  \n",
        "  MLP = Sequential()\n",
        "  MLP.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True))\n",
        "  MLP.add(Flatten())\n",
        "  MLP.add(Dense(350, input_shape=(embedding_vector_length*max_seq_length,), activation='relu'))\n",
        "  MLP.add(Dense(50, activation='relu'))\n",
        "  MLP.add(Dense(dim, activation='sigmoid'))\n",
        "  \n",
        "  \n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  MLP.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "\n",
        "  return MLP"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thirdModel = thirdModel(embedding_vector_length,32)\n",
        "thirdModel.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = thirdModel.predict(x_train[0])\n",
        "print(X_train.Claim[0])\n",
        "print(ris[0])"
      ],
      "metadata": {
        "id": "TBShaJHsZtne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSJckeXRHMNs"
      },
      "source": [
        "def fourthModel(embedding_vector_length, start_lr=0.001):\n",
        "  \n",
        "  EMB = Sequential()\n",
        "  EMB.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True))  # do not train embeddings, but we can do it\n",
        "  EMB.add(GlobalAveragePooling1D())\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  EMB.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "  return EMB"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fourthModel = fourthModel(embedding_vector_length)\n",
        "fourthModel.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = fourthModel.predict(x_train[0])\n",
        "print(X_train.Claim[0])\n",
        "print(ris[0])"
      ],
      "metadata": {
        "id": "10u4JfbsaJ_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gypl5z5ElJo1"
      },
      "source": [
        "## 5.2 Merging multi-inputs\n",
        "\n",
        "At this point, we have to think about **how** we should merge evidence and claim sentence embeddings.\n",
        "\n",
        "For simplicity, we stick to simple merging strategies:\n",
        "\n",
        "*     **Concatenation**: define the classification input as the concatenation of evidence and claim sentence embeddings\n",
        "\n",
        "*     **Sum**: define the classification input as the sum of evidence and claim sentence embeddings\n",
        "\n",
        "*     **Mean**: define the classification input as the mean of evidence and claim sentence embeddings\n",
        "\n",
        "For clarity, if the sentence embedding of a single input has shape `[batch_size, embedding_dim]`, then the classification input has shape:\n",
        "\n",
        "*     **Concatenation**: `[batch_size, 2 * embedding_dim]`\n",
        "\n",
        "*     **Sum**: `[batch_size, embedding_dim]`\n",
        "\n",
        "*     **Mean**: `[batch_size, embedding_dim]`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example to check if it is working, no train done\n",
        "emb_claim = firstModel.predict(x_train[0])[1]\n",
        "emb_evidence = firstModel.predict(x_train[1])[1]\n",
        "#1 case\n",
        "first_emb = concatenate([emb_claim[0], emb_evidence[0]])\n",
        "print(first_emb)\n",
        "#2 case\n",
        "second_emb = add([emb_claim[0], emb_evidence[0]])\n",
        "print(second_emb)\n",
        "#3 case\n",
        "third_emb = average([emb_claim[0], emb_evidence[0]])\n",
        "print(third_emb)"
      ],
      "metadata": {
        "id": "oyGIMkYtdU9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenate([embedding_c, embedding_e])"
      ],
      "metadata": {
        "id": "ZZkn2eRudShx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "Qodi5wtE16yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(tf.keras.Model):\n",
        "  def __init__(self, embedding_vector_length, dim, type_embed, type_merge, \n",
        "               cosine_similarity=False, embed_trainable=True, start_lr=0.001, **kwargs):\n",
        "    super(Classifier, self).__init__(**kwargs)\n",
        "\n",
        "    self.type_embed = type_embed\n",
        "    self.cosine_similarity = cosine_similarity\n",
        "\n",
        "    self.input_layer = Input(shape=(max_seq_length))\n",
        "    self.embedding_1 = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                        input_length=max_seq_length, \n",
        "                        trainable=embed_trainable, \n",
        "                        mask_zero=True)\n",
        "    if type_embed == \"firstModel\":\n",
        "      self.embedding_2 = SimpleRNN(dim,return_state=True)\n",
        "    elif type_embed == \"secondModel\":\n",
        "      self.embedding_2 = SimpleRNN(dim,return_sequences=True)\n",
        "      self.embedding_3 = GlobalAveragePooling1D()\n",
        "    elif type_embed == \"thirdModel\":\n",
        "      self.embedding_2 = Flatten()\n",
        "      self.embedding_3 = Dense(350, input_shape=(embedding_vector_length*max_seq_length,), activation='relu')\n",
        "      self.embedding_4 = Dense(50, activation='relu')\n",
        "      self.embedding_5 = Dense(dim, activation='sigmoid')\n",
        "    elif type_embed == \"fourthModel\":\n",
        "      self.embedding_2 = GlobalAveragePooling1D()\n",
        "\n",
        "    if type_merge == \"concat\":\n",
        "      self.merge = Concatenate()\n",
        "    elif type_merge == \"sum\":\n",
        "      self.merge = Add()\n",
        "    elif type_merge == \"mean\":\n",
        "      self.merge = Average()\n",
        "\n",
        "    self.concatenate = Concatenate()\n",
        "\n",
        "    self.cosine_similarity = Dot(axes=1, normalize=True)\n",
        "\n",
        "    self.dense_1 = Dense(128, activation=\"relu\")\n",
        "    self.dense_2 = Dense(64, activation=\"relu\")\n",
        "    self.activation = Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "  def call(self, input):\n",
        "    claim = input[0]\n",
        "    evidence = input[1]\n",
        "    c_embed = self.embedding_1(claim)\n",
        "    e_embed = self.embedding_1(evidence)\n",
        "\n",
        "    c_embed = self.embedding_2(c_embed)\n",
        "    e_embed = self.embedding_2(e_embed)\n",
        "\n",
        "    if self.type_embed == \"thirdModel\":\n",
        "      c_embed = self.embedding_3(c_embed)\n",
        "      e_embed = self.embedding_3(e_embed)\n",
        "      c_embed = self.embedding_4(c_embed)\n",
        "      e_embed = self.embedding_4(e_embed)\n",
        "      c_embed = self.embedding_5(c_embed)\n",
        "      e_embed = self.embedding_5(e_embed)\n",
        "    elif self.type_embed == \"secondModel\":\n",
        "      c_embed = self.embedding_3(c_embed)\n",
        "      e_embed = self.embedding_3(e_embed)\n",
        "\n",
        "    if self.type_embed == \"firstModel\":\n",
        "      c_embed = c_embed[1]\n",
        "      e_embed = e_embed[1]\n",
        "\n",
        "    class_input = self.merge([c_embed, e_embed])\n",
        "\n",
        "    if self.cosine_similarity:\n",
        "      cos_sim = self.cosine_similarity([c_embed, e_embed])\n",
        "      class_input = self.concatenate([class_input, cos_sim])\n",
        "\n",
        "    x = self.dense_1(class_input)\n",
        "    x = self.dense_2(x)\n",
        "    return self.activation(x)\n",
        "\n",
        "  def summary(self):\n",
        "    model = Model(inputs=[self.input_layer, self.input_layer],\n",
        "                  outputs=self.call(self.input_layer, self.input_layer))\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "wSg4lr6Rh1EA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "AUeEi0Tb6mKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.0001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "base_model = Classifier(embedding_vector_length, 32, \"firstModel\", \"mean\", \n",
        "                        embed_trainable=True, cosine_similarity=True)\n",
        "base_model.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "history = base_model.fit(x=[X_train.Claim, X_train.Evidence], y=y_train, \n",
        "                         validation_data=([X_val.Claim, X_val.Evidence], y_val), \n",
        "                         epochs=100, batch_size=64)"
      ],
      "metadata": {
        "id": "tXnZVgIom7Ub",
        "outputId": "ee55f52f-1dac-498b-c4ce-429af9565243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "57/57 [==============================] - 6s 28ms/step - loss: 0.6915 - accuracy: 0.5217 - val_loss: 0.6905 - val_accuracy: 0.5275\n",
            "Epoch 2/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.6896 - accuracy: 0.5534 - val_loss: 0.6898 - val_accuracy: 0.5316\n",
            "Epoch 3/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.6864 - accuracy: 0.5735 - val_loss: 0.6894 - val_accuracy: 0.5334\n",
            "Epoch 4/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.6809 - accuracy: 0.5924 - val_loss: 0.6896 - val_accuracy: 0.5379\n",
            "Epoch 5/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.6715 - accuracy: 0.6171 - val_loss: 0.6909 - val_accuracy: 0.5365\n",
            "Epoch 6/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.6595 - accuracy: 0.6294 - val_loss: 0.6956 - val_accuracy: 0.5376\n",
            "Epoch 7/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.6461 - accuracy: 0.6441 - val_loss: 0.7041 - val_accuracy: 0.5313\n",
            "Epoch 8/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.6336 - accuracy: 0.6544 - val_loss: 0.7131 - val_accuracy: 0.5320\n",
            "Epoch 9/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.6238 - accuracy: 0.6558 - val_loss: 0.7244 - val_accuracy: 0.5306\n",
            "Epoch 10/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.6151 - accuracy: 0.6594 - val_loss: 0.7348 - val_accuracy: 0.5285\n",
            "Epoch 11/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.6079 - accuracy: 0.6625 - val_loss: 0.7413 - val_accuracy: 0.5306\n",
            "Epoch 12/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.6017 - accuracy: 0.6630 - val_loss: 0.7505 - val_accuracy: 0.5271\n",
            "Epoch 13/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.5963 - accuracy: 0.6622 - val_loss: 0.7571 - val_accuracy: 0.5306\n",
            "Epoch 14/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5907 - accuracy: 0.6672 - val_loss: 0.7660 - val_accuracy: 0.5296\n",
            "Epoch 15/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5858 - accuracy: 0.6694 - val_loss: 0.7714 - val_accuracy: 0.5264\n",
            "Epoch 16/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5813 - accuracy: 0.6694 - val_loss: 0.7773 - val_accuracy: 0.5337\n",
            "Epoch 17/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5759 - accuracy: 0.6789 - val_loss: 0.7865 - val_accuracy: 0.5309\n",
            "Epoch 18/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5718 - accuracy: 0.6856 - val_loss: 0.7949 - val_accuracy: 0.5327\n",
            "Epoch 19/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5667 - accuracy: 0.6912 - val_loss: 0.8023 - val_accuracy: 0.5261\n",
            "Epoch 20/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5627 - accuracy: 0.6953 - val_loss: 0.8090 - val_accuracy: 0.5275\n",
            "Epoch 21/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5589 - accuracy: 0.6987 - val_loss: 0.8175 - val_accuracy: 0.5309\n",
            "Epoch 22/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5547 - accuracy: 0.6970 - val_loss: 0.8250 - val_accuracy: 0.5275\n",
            "Epoch 23/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5510 - accuracy: 0.7028 - val_loss: 0.8345 - val_accuracy: 0.5316\n",
            "Epoch 24/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5482 - accuracy: 0.6987 - val_loss: 0.8415 - val_accuracy: 0.5289\n",
            "Epoch 25/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5449 - accuracy: 0.7034 - val_loss: 0.8481 - val_accuracy: 0.5292\n",
            "Epoch 26/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5423 - accuracy: 0.7040 - val_loss: 0.8581 - val_accuracy: 0.5278\n",
            "Epoch 27/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5406 - accuracy: 0.7034 - val_loss: 0.8627 - val_accuracy: 0.5285\n",
            "Epoch 28/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5384 - accuracy: 0.7012 - val_loss: 0.8713 - val_accuracy: 0.5254\n",
            "Epoch 29/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5361 - accuracy: 0.7053 - val_loss: 0.8752 - val_accuracy: 0.5219\n",
            "Epoch 30/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5350 - accuracy: 0.7031 - val_loss: 0.8819 - val_accuracy: 0.5202\n",
            "Epoch 31/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5326 - accuracy: 0.7037 - val_loss: 0.8888 - val_accuracy: 0.5170\n",
            "Epoch 32/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5314 - accuracy: 0.7031 - val_loss: 0.8934 - val_accuracy: 0.5205\n",
            "Epoch 33/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5302 - accuracy: 0.7053 - val_loss: 0.8963 - val_accuracy: 0.5177\n",
            "Epoch 34/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5287 - accuracy: 0.7028 - val_loss: 0.9015 - val_accuracy: 0.5191\n",
            "Epoch 35/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5272 - accuracy: 0.7042 - val_loss: 0.9069 - val_accuracy: 0.5188\n",
            "Epoch 36/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5257 - accuracy: 0.7026 - val_loss: 0.9145 - val_accuracy: 0.5195\n",
            "Epoch 37/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5247 - accuracy: 0.7053 - val_loss: 0.9207 - val_accuracy: 0.5170\n",
            "Epoch 38/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5236 - accuracy: 0.7042 - val_loss: 0.9243 - val_accuracy: 0.5174\n",
            "Epoch 39/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5226 - accuracy: 0.7026 - val_loss: 0.9273 - val_accuracy: 0.5195\n",
            "Epoch 40/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5215 - accuracy: 0.7020 - val_loss: 0.9313 - val_accuracy: 0.5170\n",
            "Epoch 41/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5206 - accuracy: 0.7020 - val_loss: 0.9338 - val_accuracy: 0.5136\n",
            "Epoch 42/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5195 - accuracy: 0.7078 - val_loss: 0.9391 - val_accuracy: 0.5146\n",
            "Epoch 43/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5188 - accuracy: 0.7059 - val_loss: 0.9456 - val_accuracy: 0.5156\n",
            "Epoch 44/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5185 - accuracy: 0.6998 - val_loss: 0.9449 - val_accuracy: 0.5143\n",
            "Epoch 45/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5171 - accuracy: 0.7037 - val_loss: 0.9515 - val_accuracy: 0.5136\n",
            "Epoch 46/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5161 - accuracy: 0.7017 - val_loss: 0.9581 - val_accuracy: 0.5150\n",
            "Epoch 47/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5158 - accuracy: 0.7028 - val_loss: 0.9595 - val_accuracy: 0.5073\n",
            "Epoch 48/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5152 - accuracy: 0.7056 - val_loss: 0.9620 - val_accuracy: 0.5177\n",
            "Epoch 49/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5139 - accuracy: 0.7053 - val_loss: 0.9661 - val_accuracy: 0.5170\n",
            "Epoch 50/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5135 - accuracy: 0.7034 - val_loss: 0.9688 - val_accuracy: 0.5129\n",
            "Epoch 51/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5133 - accuracy: 0.7003 - val_loss: 0.9747 - val_accuracy: 0.5150\n",
            "Epoch 52/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5125 - accuracy: 0.7017 - val_loss: 0.9756 - val_accuracy: 0.5139\n",
            "Epoch 53/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5124 - accuracy: 0.7034 - val_loss: 0.9771 - val_accuracy: 0.5160\n",
            "Epoch 54/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5114 - accuracy: 0.7053 - val_loss: 0.9835 - val_accuracy: 0.5163\n",
            "Epoch 55/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5118 - accuracy: 0.7003 - val_loss: 0.9862 - val_accuracy: 0.5143\n",
            "Epoch 56/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5101 - accuracy: 0.7001 - val_loss: 0.9900 - val_accuracy: 0.5163\n",
            "Epoch 57/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5097 - accuracy: 0.7048 - val_loss: 0.9940 - val_accuracy: 0.5125\n",
            "Epoch 58/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5092 - accuracy: 0.7026 - val_loss: 0.9978 - val_accuracy: 0.5174\n",
            "Epoch 59/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.5088 - accuracy: 0.7042 - val_loss: 0.9978 - val_accuracy: 0.5125\n",
            "Epoch 60/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5092 - accuracy: 0.6995 - val_loss: 1.0024 - val_accuracy: 0.5184\n",
            "Epoch 61/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5094 - accuracy: 0.6973 - val_loss: 1.0024 - val_accuracy: 0.5125\n",
            "Epoch 62/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5078 - accuracy: 0.7026 - val_loss: 1.0052 - val_accuracy: 0.5136\n",
            "Epoch 63/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5080 - accuracy: 0.7042 - val_loss: 1.0105 - val_accuracy: 0.5132\n",
            "Epoch 64/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5069 - accuracy: 0.6998 - val_loss: 1.0121 - val_accuracy: 0.5136\n",
            "Epoch 65/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5071 - accuracy: 0.6987 - val_loss: 1.0151 - val_accuracy: 0.5146\n",
            "Epoch 66/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5066 - accuracy: 0.6984 - val_loss: 1.0158 - val_accuracy: 0.5146\n",
            "Epoch 67/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5065 - accuracy: 0.7034 - val_loss: 1.0210 - val_accuracy: 0.5087\n",
            "Epoch 68/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5066 - accuracy: 0.6984 - val_loss: 1.0224 - val_accuracy: 0.5108\n",
            "Epoch 69/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5059 - accuracy: 0.7014 - val_loss: 1.0216 - val_accuracy: 0.5104\n",
            "Epoch 70/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5058 - accuracy: 0.6978 - val_loss: 1.0244 - val_accuracy: 0.5143\n",
            "Epoch 71/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5053 - accuracy: 0.7017 - val_loss: 1.0279 - val_accuracy: 0.5090\n",
            "Epoch 72/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5046 - accuracy: 0.7009 - val_loss: 1.0322 - val_accuracy: 0.5083\n",
            "Epoch 73/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5048 - accuracy: 0.6998 - val_loss: 1.0345 - val_accuracy: 0.5083\n",
            "Epoch 74/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5044 - accuracy: 0.6995 - val_loss: 1.0380 - val_accuracy: 0.5108\n",
            "Epoch 75/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5037 - accuracy: 0.6978 - val_loss: 1.0417 - val_accuracy: 0.5167\n",
            "Epoch 76/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5038 - accuracy: 0.7037 - val_loss: 1.0439 - val_accuracy: 0.5104\n",
            "Epoch 77/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5031 - accuracy: 0.6981 - val_loss: 1.0456 - val_accuracy: 0.5087\n",
            "Epoch 78/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5032 - accuracy: 0.7006 - val_loss: 1.0538 - val_accuracy: 0.5170\n",
            "Epoch 79/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5037 - accuracy: 0.7026 - val_loss: 1.0495 - val_accuracy: 0.5101\n",
            "Epoch 80/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5029 - accuracy: 0.7006 - val_loss: 1.0519 - val_accuracy: 0.5136\n",
            "Epoch 81/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5027 - accuracy: 0.7026 - val_loss: 1.0570 - val_accuracy: 0.5129\n",
            "Epoch 82/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.5024 - accuracy: 0.7003 - val_loss: 1.0588 - val_accuracy: 0.5118\n",
            "Epoch 83/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5024 - accuracy: 0.6987 - val_loss: 1.0623 - val_accuracy: 0.5115\n",
            "Epoch 84/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5024 - accuracy: 0.6987 - val_loss: 1.0608 - val_accuracy: 0.5132\n",
            "Epoch 85/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.5021 - accuracy: 0.7014 - val_loss: 1.0707 - val_accuracy: 0.5174\n",
            "Epoch 86/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5027 - accuracy: 0.6984 - val_loss: 1.0666 - val_accuracy: 0.5129\n",
            "Epoch 87/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5018 - accuracy: 0.7006 - val_loss: 1.0698 - val_accuracy: 0.5122\n",
            "Epoch 88/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.5016 - accuracy: 0.6967 - val_loss: 1.0685 - val_accuracy: 0.5101\n",
            "Epoch 89/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5013 - accuracy: 0.7017 - val_loss: 1.0723 - val_accuracy: 0.5045\n",
            "Epoch 90/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5015 - accuracy: 0.7062 - val_loss: 1.0717 - val_accuracy: 0.5111\n",
            "Epoch 91/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5015 - accuracy: 0.6984 - val_loss: 1.0760 - val_accuracy: 0.5146\n",
            "Epoch 92/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5008 - accuracy: 0.7040 - val_loss: 1.0782 - val_accuracy: 0.5097\n",
            "Epoch 93/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5010 - accuracy: 0.7017 - val_loss: 1.0785 - val_accuracy: 0.5087\n",
            "Epoch 94/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5006 - accuracy: 0.7001 - val_loss: 1.0827 - val_accuracy: 0.5090\n",
            "Epoch 95/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5007 - accuracy: 0.6970 - val_loss: 1.0849 - val_accuracy: 0.5090\n",
            "Epoch 96/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.5006 - accuracy: 0.7020 - val_loss: 1.0849 - val_accuracy: 0.5146\n",
            "Epoch 97/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5008 - accuracy: 0.7051 - val_loss: 1.0947 - val_accuracy: 0.5139\n",
            "Epoch 98/100\n",
            "57/57 [==============================] - 1s 17ms/step - loss: 0.5003 - accuracy: 0.6981 - val_loss: 1.0889 - val_accuracy: 0.5063\n",
            "Epoch 99/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.5001 - accuracy: 0.6998 - val_loss: 1.0914 - val_accuracy: 0.5160\n",
            "Epoch 100/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.4996 - accuracy: 0.7020 - val_loss: 1.0931 - val_accuracy: 0.5122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvH6c2z00T4o",
        "outputId": "bc7df0f5-2ccf-49b4-ca1d-9f69beb9f576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhVg9ZLymOUc"
      },
      "source": [
        "# A simple extension\n",
        "\n",
        "Lastly, we ask you to modify previously defined neural architectures by adding an additional feature to the classification input.\n",
        "\n",
        "We would like to see if some similarity information between the claim to verify and one of its associated evidence might be useful to the classification.\n",
        "\n",
        "Compute the cosine similarity metric between the two sentence embeddings and concatenate the result to the classification input.\n",
        "\n",
        "For clarity, since the cosine similarity of two vectors outputs a scalar value, the classification input shape is modified as follows:\n",
        "\n",
        "*     **Concatenation**: `[batch_size, 2 * embedding_dim + 1]`\n",
        "\n",
        "*     **Sum**: `[batch_size, embedding_dim + 1]`\n",
        "\n",
        "*     **Mean**: `[batch_size, embedding_dim + 1]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd74ULgpnJrc"
      },
      "source": [
        "# Performance evaluation\n",
        "\n",
        "Due to our simplifications, obtained results are not directly compatible with a traditional fact checking method that considers the evidence set as a whole.\n",
        "\n",
        "Thus, we need to consider two types of evaluations.\n",
        "\n",
        "---\n",
        "\n",
        "A. **Multi-input classification evaluation**\n",
        "\n",
        "This type of evaluation is the easiest and concerns computing evaluation metrics, such as accuracy, f1-score, recall and precision, of our pre-processed dataset.\n",
        "\n",
        "In other words, we assess the performance of chosen classifiers.\n",
        "\n",
        "---\n",
        "\n",
        "B. **Claim verification evaluation**\n",
        "\n",
        "However, if we want to give an answer concerning the claim itself, we need to consider the whole evidence set. \n",
        "\n",
        "Intuitively, for a given claim, we consider all its corresponding (claim, evidence) pairs and their corresponding classification outputs. \n",
        "\n",
        "At this point, all we need to do is to compute the final predicted claim label via majority voting.\n",
        "\n",
        "---\n",
        "\n",
        "Example:\n",
        "\n",
        "    Claim: c1\n",
        "    Evidence set: e1, e2, e3\n",
        "    True label: S\n",
        "\n",
        "    Pair outputs:\n",
        "    (c1, e1) -> S (supports)\n",
        "    (c1, e2) -> S (supports)\n",
        "    (c1, e3) -> R (refutes)\n",
        "\n",
        "    Majority voting:\n",
        "    S -> 2 votes\n",
        "    R -> 1 vote\n",
        "\n",
        "    Final label:\n",
        "    c1 -> S\n",
        "\n",
        "Lastly, we have to compute classification metrics just like before.\n",
        "\n",
        "Shortly speaking, implement both strategies for your classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A Multi input classification evaluation\n",
        "def inputClassificationEvaluation(y,predictions):\n",
        "  return classification_report(np.array(y), np.array(predictions), \n",
        "                               target_names=['refutes','supports'],\n",
        "                               labels=[0,1]);\n",
        "\n",
        "\n",
        "#B Claim verification evaluation\n",
        "def claim_verification_evaluation(X,y,predictions):\n",
        "  y_final=[]\n",
        "  predictions_final=[]\n",
        "  ris_label = -1\n",
        "  ris_predicted = -1\n",
        "\n",
        "  X['Label'] = y\n",
        "  X['Predicted'] = predictions\n",
        "  claims =  X.Claim.unique()\n",
        "\n",
        "  for el in claims:\n",
        "    #get every row with same Claim\n",
        "    rows = X.loc[X['Claim'] == el]\n",
        "    #get an array of the real label\n",
        "    label = np.array(rows.Label)\n",
        "    #get an array with the predictions\n",
        "    predicted = np.array(rows.Predicted)\n",
        "    #check the higher number of vote\n",
        "    if sum(label) >= label.size/2:\n",
        "      ris_label = 1\n",
        "    else:\n",
        "      ris_label = 0\n",
        "    if sum(predicted) >=  predicted.size/2:\n",
        "      ris_predicted = 1\n",
        "    else:\n",
        "      ris_predicted = 0\n",
        "    #append to the final result \n",
        "    y_final.append(ris_label)\n",
        "    predictions_final.append(ris_predicted)\n",
        "  return inputClassificationEvaluation(y_final,predictions_final)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SwSFFQDXApbn"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4LJ2yPxsUOV"
      },
      "source": [
        "# Tips and Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf80UVRNrXve"
      },
      "source": [
        "## 8.1 Extensions are welcome!\n",
        "\n",
        "Is this task too easy for you? Are you curious to try out things you have seen during lectures (e.g. attention)? Feel free to try everything you want!\n",
        "\n",
        "**Don't forget to try neural baselines first!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COXeCXdYsBEf"
      },
      "source": [
        "## 8.2 Comments and documentation\n",
        "\n",
        "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejv6SDE8xc4_"
      },
      "source": [
        "## 8.3 Organization\n",
        "\n",
        "We suggest you to divide your work into sections. This allows you to build clean and modular code, as well as easy to read and to debug.\n",
        "\n",
        "A possible schema:\n",
        "\n",
        "*   Dataset pre-processing\n",
        "*   Dataset conversion\n",
        "*   Model definition\n",
        "*   Training\n",
        "*   Evaluation\n",
        "*   Comments/Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19QWjgGzIKOq"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Which are the evaluation criteria on which we'll judge you and your work?\n",
        "\n",
        "1. Pre-processing: whether you have done some pre-processing or not.\n",
        "2. Sentence embedding: you should implement all required strategies (with an example and working code for each). That is, we, as evaluators, should be able to test all strategies without writing down new code.\n",
        "3. Multiple inputs merging strategies: you should implement all required strategies (with an example and working code for each).\n",
        "4. Similarity extension: you should implement the cosine similarity extension (with an example and working code).\n",
        "5. Voting strategy: you should implement the majority voting strategy and provide results.\n",
        "6. Report: when submitting your notebook, you should also attach a small summary report that describes what you have done (provide motivations as well for abitrary steps. For instance, \"We've applied L2 regularization since the model was overfitting\".\n",
        "\n",
        "Extras (possible extra points):\n",
        "\n",
        "1. Any well defined extension is welcome!\n",
        "2. Well organized and commented code is as important as any other criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DR70uh7pabo"
      },
      "source": [
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "*Note*: We highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information before contacting us!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc0gNWU2pgKQ"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Can I do something text pre-processing?\n",
        "\n",
        "**Answer:** You have to! If you check text data, the majority of sentences need some cleaning.\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: The model architecture schema is not so clear, are we doing end-to-end training?\n",
        "\n",
        "**Answer**: Exactly! All models can be thought as:\n",
        "\n",
        "1. Input\n",
        "2. (word) Embedding\n",
        "3. Sentence embedding\n",
        "4. Multiple inputs merging\n",
        "5. Classification\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Can I extend models by adding more layers?\n",
        "\n",
        "**Answer**: Feel free to define model architectures as you wish, but remember satisfy our requirements. This assignment should not be thought as a competition to achieve the best performing model: fancy students that want to show off but miss required assignment objectives will be punished!!\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: I'm struggling with the implementation. Can you help me?\n",
        "\n",
        "**Answer**: Yes sure! Contact us and describe your issue. If you are looking for a particular type of operation, you can easily check the documentation of the deep learning framework you are using (google is your friend).\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Can I try other encoding strategies or neural architectures?\n",
        "\n",
        "**Answer:** Absolutely! Remember to try out recommended neural baselines first and only then proceed with your extensions.\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Do we have to test all possible sentence embedding and input merging combinations?\n",
        "\n",
        "**Answer**: Absolutely no! Feel free to pick one sentence embedding strategy and try all possible input merging strategies with it! For instance, pick the best performing sentence embedding method and proceed with next steps (extras included). Please, note that you still have to implement all mentioned strategies!\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: I'm hitting out of memory error when training my models, do you have any suggestions?\n",
        "\n",
        "**Answer**: Here are some common workarounds:\n",
        "\n",
        "1. Try decreasing the mini-batch size\n",
        "2. Try applying a different padding strategy (if you are applying padding): e.g. use quantiles instead of maximum sequence length\n",
        "3. Check the efficiency of your custom code implementation (if any)\n",
        "4. Try to define same length mini-batches to avoid padding (**It should not be necessary here!**)\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: I'm hitting CUDNN_STATUS_BAD_PARAM error! What I'm doing wrong?\n",
        "\n",
        "**Answer**: This error is a little bit tricky since the stack trace is not meaningful at all! This error occurs when the RNN is fed with a sequence of all 0s and pad masking is enabled (e.g. from the embedding layer). Please, check your conversion step, since there might be an error that leads to the encoding of a sentence to all 0s.\n",
        "\n",
        "---"
      ]
    }
  ]
}